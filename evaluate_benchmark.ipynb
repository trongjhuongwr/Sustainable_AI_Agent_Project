{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f3e735",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# EVALUATE BENCHMARK NOTEBOOK\n",
    "# =============================================================================\n",
    "    Purpose:\n",
    "        Load baseline model, trained agent (or best action), test data.\n",
    "        Apply baseline, manual, and agent optimizations.\n",
    "        Run comprehensive benchmark and visualize results.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b880f",
   "metadata": {},
   "source": [
    "# Import Configuration and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ[\"GYM_DISABLE_WARNINGS\"] = \"true\"\n",
    "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"stable_baselines3\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym # Might be needed if loading agent requires env instance\n",
    "from gymnasium import spaces # Might be needed if loading agent requires env instance\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from stable_baselines3 import PPO # Needed if loading agent directly\n",
    "from codecarbon import EmissionsTracker\n",
    "from ptflops import get_model_complexity_info\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from builtins import print as builtin_print # To avoid conflict\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2120cde",
   "metadata": {},
   "source": [
    "# Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths for Loading\n",
    "    PROCESSED_DATA_PATH = '/kaggle/working/processed_data.pt'\n",
    "    BASELINE_MODEL_PATH = '/kaggle/working/baseline_model.pth'\n",
    "    AGENT_SAVE_PATH = \"/kaggle/working/sustainable_ai_agent_ppo.zip\"\n",
    "    BEST_ACTION_PATH = \"/kaggle/working/best_action.json\"\n",
    "\n",
    "    # Data Parameters\n",
    "    SEQUENCE_LENGTH = 30\n",
    "    INPUT_DIM = 4\n",
    "\n",
    "    # Model Parameters (must match baseline)\n",
    "    HIDDEN_DIM = 256\n",
    "    N_LAYERS = 2\n",
    "    OUTPUT_DIM = 1\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    # Evaluation Parameters\n",
    "    EVAL_BATCH_SIZE = 64\n",
    "    CODECARBON_BATCHES = 10\n",
    "\n",
    "    # Device (Evaluation mostly on CPU, but consistency)\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device for potential agent loading: {Config.DEVICE}\")\n",
    "print(f\"Loading data from: {Config.PROCESSED_DATA_PATH}\")\n",
    "print(f\"Loading baseline model from: {Config.BASELINE_MODEL_PATH}\")\n",
    "print(f\"Loading agent from: {Config.AGENT_SAVE_PATH}\")\n",
    "print(f\"Loading best action from: {Config.BEST_ACTION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa52a2",
   "metadata": {},
   "source": [
    "# Helper Functions and Model Definition\n",
    "# (Copied again for standalone execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Counts trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def evaluate_model(model, loader, codecarbon_batches=10):\n",
    "    \"\"\"Comprehensive evaluation (accuracy, params, flops, energy). Runs on CPU.\"\"\"\n",
    "    model_cpu = copy.deepcopy(model).cpu()\n",
    "    device = torch.device(\"cpu\")\n",
    "    model_cpu.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    # Accuracy\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model_cpu(inputs)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Energy and CO2 with CodeCarbon\n",
    "    energy_kwh = 0\n",
    "    co2_eq_kg = 0\n",
    "    try:\n",
    "        tracker = EmissionsTracker(log_level=\"error\", output_dir=\"/kaggle/working/\", tracking_mode=\"process\")\n",
    "        tracker.start()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, _) in enumerate(loader):\n",
    "                if i >= codecarbon_batches: break\n",
    "                model_cpu(inputs.to(device))\n",
    "        tracker.stop()\n",
    "        if tracker.final_emissions_data:\n",
    "             energy_kwh = tracker.final_emissions_data.energy_consumed or 0\n",
    "             co2_eq_kg = tracker.final_emissions_data.emissions or 0\n",
    "        else:\n",
    "            builtin_print(\"Warning: CodeCarbon tracker did not record final emissions data.\")\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Warning: CodeCarbon measurement failed - {e}\")\n",
    "\n",
    "    # Params\n",
    "    params = count_parameters(model_cpu)\n",
    "\n",
    "    # FLOPs with ptflops\n",
    "    flops = 0\n",
    "    try:\n",
    "        # Create a dummy input on CPU for ptflops\n",
    "        dummy_input = torch.randn(1, Config.SEQUENCE_LENGTH, Config.INPUT_DIM).to(device)\n",
    "        macs, _ = get_model_complexity_info(\n",
    "            model_cpu, (Config.SEQUENCE_LENGTH, Config.INPUT_DIM),\n",
    "            input_constructor=lambda x: {'x': dummy_input}, # Provide input via constructor if needed\n",
    "            as_strings=False, print_per_layer_stat=False, verbose=False)\n",
    "        flops = macs * 2\n",
    "    except (KeyError, AttributeError, RuntimeError, TypeError):\n",
    "        # builtin_print(\"Note: Could not calculate FLOPs via ptflops (likely quantized model). Reporting 0.\")\n",
    "        flops = 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \"energy_kwh\": energy_kwh, \"co2_eq_kg\": co2_eq_kg,\n",
    "        \"flops\": flops, \"params\": params,\n",
    "    }\n",
    "\n",
    "def apply_l1_pruning(model, amount):\n",
    "    \"\"\"Applies L1 pruning, ignoring GRU layers. Returns CPU model.\"\"\"\n",
    "    model_to_prune = copy.deepcopy(model).cpu()\n",
    "    model_to_prune.eval()\n",
    "    ignored_layers = [m for m in model_to_prune.modules() if isinstance(m, nn.GRU)]\n",
    "    example_inputs = torch.randn(1, Config.SEQUENCE_LENGTH, Config.INPUT_DIM)\n",
    "    try:\n",
    "        pruner = tp.pruner.MagnitudePruner(\n",
    "            model_to_prune,\n",
    "            example_inputs=example_inputs,\n",
    "            importance=tp.importance.MagnitudeImportance(p=1),\n",
    "            pruning_ratio=amount,\n",
    "            ignored_layers=ignored_layers\n",
    "        )\n",
    "        pruner.step()\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Error during pruning: {e}\")\n",
    "        # Return the unpruned model copy in case of error\n",
    "    return model_to_prune\n",
    "\n",
    "def apply_dynamic_quantization(model):\n",
    "    \"\"\"Applies dynamic quantization. Returns CPU model.\"\"\"\n",
    "    try:\n",
    "        quantized_model = copy.deepcopy(model).cpu()\n",
    "        quantized_model.eval()\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            quantized_model, {nn.GRU, nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "        return quantized_model\n",
    "    except Exception as e:\n",
    "         builtin_print(f\"Error during dynamic quantization: {e}\")\n",
    "         # Return the original model copy in case of error\n",
    "         return copy.deepcopy(model).cpu()\n",
    "\n",
    "\n",
    "class WeatherGRU(nn.Module):\n",
    "    \"\"\"GRU model definition.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim, dropout):\n",
    "        super(WeatherGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers,\n",
    "                          batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "print(\"Helper functions and WeatherGRU class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be397e6b",
   "metadata": {},
   "source": [
    "# Load Baseline Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed test data\n",
    "try:\n",
    "    processed_data = torch.load(Config.PROCESSED_DATA_PATH)\n",
    "    X_test_tensor = processed_data['X_test']\n",
    "    y_test_tensor = processed_data['y_test']\n",
    "    print(f\"Processed test data loaded from {Config.PROCESSED_DATA_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data file not found at {Config.PROCESSED_DATA_PATH}. Run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processed data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create Test DataLoader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.EVAL_BATCH_SIZE, shuffle=False)\n",
    "print(\"Test DataLoader created.\")\n",
    "\n",
    "# Initialize and load baseline model\n",
    "baseline_model = WeatherGRU(\n",
    "    input_dim=Config.INPUT_DIM,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    n_layers=Config.N_LAYERS,\n",
    "    output_dim=Config.OUTPUT_DIM,\n",
    "    dropout=Config.DROPOUT\n",
    ")\n",
    "try:\n",
    "    baseline_model.load_state_dict(torch.load(Config.BASELINE_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "    baseline_model.eval()\n",
    "    print(f\"Baseline model state loaded from {Config.BASELINE_MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Baseline model file not found at {Config.BASELINE_MODEL_PATH}. Run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading baseline model state: {e}\")\n",
    "    raise\n",
    "\n",
    "# Keep a clean copy on CPU\n",
    "baseline_model_cpu = baseline_model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b6f05",
   "metadata": {},
   "source": [
    "# Determine Best Action from Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b70792",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solution_info = None\n",
    "\n",
    "# Try loading from JSON first (more efficient)\n",
    "try:\n",
    "    with open(Config.BEST_ACTION_PATH, 'r') as f:\n",
    "        best_solution_info = json.load(f)\n",
    "        # Validate keys needed for benchmark function\n",
    "        if 'pruning_amount' in best_solution_info and 'quantized' in best_solution_info:\n",
    "             print(f\"Best action info loaded from {Config.BEST_ACTION_PATH}:\")\n",
    "             print(f\"  Pruning: {best_solution_info['pruning_amount']*100:.0f}%\")\n",
    "             print(f\"  Quantized: {best_solution_info['quantized']}\")\n",
    "        else:\n",
    "             print(f\"JSON file {Config.BEST_ACTION_PATH} does not contain required keys ('pruning_amount', 'quantized').\")\n",
    "             best_solution_info = None # Reset if invalid\n",
    "except FileNotFoundError:\n",
    "    print(f\"Info: Best action JSON file not found at {Config.BEST_ACTION_PATH}. Will try loading agent.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading best action JSON: {e}. Will try loading agent.\")\n",
    "\n",
    "# If JSON loading failed or file not found, load agent and predict\n",
    "if best_solution_info is None:\n",
    "    print(f\"\\n--- Loading trained agent from {Config.AGENT_SAVE_PATH} to predict best action ---\")\n",
    "    try:\n",
    "        # We need a dummy env instance to load the agent if not passed explicitly\n",
    "        # Define necessary parameters for dummy env init\n",
    "        dummy_pruning_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        dummy_action_space = spaces.Discrete(len(dummy_pruning_levels) * 2)\n",
    "        dummy_obs_space = spaces.Box(low=np.array([0.0, -1.0, 0.0, 0.0]), high=np.array([1.0, 1.0, 1.0, 1.0]), dtype=np.float32)\n",
    "\n",
    "        # Minimal mock environment class just for loading if SustainableAIAgentEnv not defined\n",
    "        class MockEnv(gym.Env):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.action_space = dummy_action_space\n",
    "                self.observation_space = dummy_obs_space\n",
    "            def step(self, action): pass\n",
    "            def reset(self, seed=None, options=None):\n",
    "                 super().reset(seed=seed)\n",
    "                 # Return a plausible initial state matching the space\n",
    "                 return np.array([0.6, 0.0, 0.0, 0.0], dtype=np.float32), {}\n",
    "            def render(self): pass\n",
    "            def close(self): pass\n",
    "\n",
    "        mock_env_instance = MockEnv()\n",
    "\n",
    "        # Load agent - ensure device matches where it was trained or use 'auto'/'cpu'\n",
    "        agent = PPO.load(Config.AGENT_SAVE_PATH, env=mock_env_instance, device='cpu') # Load to CPU for prediction\n",
    "        print(\"Agent loaded successfully.\")\n",
    "\n",
    "        # Get initial observation for prediction (use mock env reset)\n",
    "        obs, _ = mock_env_instance.reset()\n",
    "\n",
    "        # Predict the best action\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        action_item = action.item()\n",
    "        print(f\"Agent predicted best action: {action_item}\")\n",
    "\n",
    "        # Decode action\n",
    "        pruning_amount = dummy_pruning_levels[action_item % len(dummy_pruning_levels)]\n",
    "        apply_quant = (action_item >= len(dummy_pruning_levels))\n",
    "\n",
    "        best_solution_info = {\n",
    "            'pruning_amount': pruning_amount,\n",
    "            'quantized': apply_quant\n",
    "        }\n",
    "        print(f\"  Decoded Action -> Pruning: {pruning_amount*100:.0f}%, Quantized: {apply_quant}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Agent file not found at {Config.AGENT_SAVE_PATH}. Cannot evaluate agent solution.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or predicting from agent: {e}. Cannot evaluate agent solution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d15739",
   "metadata": {},
   "source": [
    "# Run Full Benchmark Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_benchmark(baseline_model_cpu, test_loader, best_action_info):\n",
    "    \"\"\"Runs benchmark comparing Baseline, Manual Pruning, Manual Quantization, and Agent Optimized.\"\"\"\n",
    "    results = {}\n",
    "    print(\"\\n--- Starting Full Benchmark Evaluation ---\")\n",
    "\n",
    "    # 1. Baseline Evaluation\n",
    "    print(\"--- 1. Evaluating Baseline Model ---\")\n",
    "    try:\n",
    "        # Evaluate a fresh copy to ensure no side effects\n",
    "        results['Baseline'] = evaluate_model(copy.deepcopy(baseline_model_cpu), test_loader, Config.CODECARBON_BATCHES)\n",
    "        print(\"  Baseline evaluation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating baseline: {e}\")\n",
    "        results['Baseline'] = {}\n",
    "\n",
    "    # 2. Manual Pruning (50%) Evaluation\n",
    "    print(\"--- 2. Evaluating Manual Pruning (50%) ---\")\n",
    "    try:\n",
    "        pruned_model_manual = apply_l1_pruning(baseline_model_cpu, 0.5)\n",
    "        results['Manual Pruning (50%)'] = evaluate_model(pruned_model_manual, test_loader, Config.CODECARBON_BATCHES)\n",
    "        print(\"  Manual pruning evaluation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating manual pruning: {e}\")\n",
    "        results['Manual Pruning (50%)'] = {}\n",
    "\n",
    "    # 3. Manual Quantization Evaluation\n",
    "    print(\"--- 3. Evaluating Manual Quantization ---\")\n",
    "    try:\n",
    "        quantized_model_manual = apply_dynamic_quantization(baseline_model_cpu)\n",
    "        results['Manual Quantization'] = evaluate_model(quantized_model_manual, test_loader, Config.CODECARBON_BATCHES)\n",
    "        print(\"  Manual quantization evaluation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating manual quantization: {e}\")\n",
    "        results['Manual Quantization'] = {}\n",
    "\n",
    "    # 4. Agent Optimized Evaluation\n",
    "    print(\"--- 4. Evaluating Agent Optimized Model ---\")\n",
    "    if best_action_info and 'pruning_amount' in best_action_info and 'quantized' in best_action_info:\n",
    "        pruning_amount_agent = best_action_info['pruning_amount']\n",
    "        apply_quant_agent = best_action_info['quantized']\n",
    "        print(f\"  Applying Agent's strategy: Pruning {pruning_amount_agent*100:.0f}%, Quantized: {apply_quant_agent}\")\n",
    "        try:\n",
    "            agent_optimized_model = copy.deepcopy(baseline_model_cpu) # Start fresh\n",
    "            if pruning_amount_agent > 0:\n",
    "                agent_optimized_model = apply_l1_pruning(agent_optimized_model, pruning_amount_agent)\n",
    "            if apply_quant_agent:\n",
    "                agent_optimized_model = apply_dynamic_quantization(agent_optimized_model)\n",
    "\n",
    "            results['Agent Optimized'] = evaluate_model(agent_optimized_model, test_loader, Config.CODECARBON_BATCHES)\n",
    "            print(\"  Agent optimized evaluation complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error evaluating agent optimized model: {e}\")\n",
    "            results['Agent Optimized'] = {}\n",
    "    else:\n",
    "        print(\"  Skipping agent evaluation: Best action info not available or invalid.\")\n",
    "        results['Agent Optimized'] = {}\n",
    "\n",
    "    # --- Format Results ---\n",
    "    df_results = pd.DataFrame(results).T.fillna(0) # Transpose and fill NaNs if errors occurred\n",
    "\n",
    "    # Calculate derived metrics (handle potential missing columns)\n",
    "    df_results['flops_g'] = (df_results['flops'] / 1e9).round(6) if 'flops' in df_results else 0\n",
    "    df_results['params_m'] = (df_results['params'] / 1e6).round(6) if 'params' in df_results else 0\n",
    "    df_results['energy_mwh'] = (df_results['energy_kwh'] * 1000).round(6) if 'energy_kwh' in df_results else 0\n",
    "\n",
    "    # Ensure required columns exist, adding them with 0 if missing\n",
    "    final_columns = ['accuracy', 'params_m', 'flops_g', 'energy_mwh', 'co2_eq_kg']\n",
    "    for col in final_columns:\n",
    "        if col not in df_results:\n",
    "            df_results[col] = 0.0 # Add missing columns with default value\n",
    "\n",
    "    # Ensure correct data types (especially after fillna)\n",
    "    for col in ['accuracy', 'params_m', 'flops_g', 'energy_mwh', 'co2_eq_kg']:\n",
    "         if col in df_results:\n",
    "             df_results[col] = pd.to_numeric(df_results[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Benchmark Evaluation Finished ---\")\n",
    "    return df_results[final_columns] # Return only the desired, formatted columns\n",
    "\n",
    "# Execute the benchmark\n",
    "benchmark_results = run_full_benchmark(baseline_model_cpu, test_loader, best_solution_info)\n",
    "\n",
    "# Display the results table\n",
    "print(\"\\n\\n--- FINAL COMPREHENSIVE BENCHMARK RESULTS ---\")\n",
    "# Use markdown format for better display in Jupyter/Kaggle\n",
    "print(benchmark_results.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622fdfc1",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12)) # Adjusted size slightly\n",
    "fig.suptitle('Comprehensive Model Performance Comparison', fontsize=18, weight='bold')\n",
    "\n",
    "# Metrics and their corresponding axes and palettes\n",
    "metrics_plot_info = [\n",
    "    ('accuracy', axes[0, 0], 'Greens_r', 'Model Accuracy', 'Accuracy Score'),\n",
    "    ('params_m', axes[0, 1], 'Blues_r', 'Model Size (Millions of Parameters)', 'Parameters (M)'),\n",
    "    ('flops_g', axes[1, 0], 'Oranges_r', 'Computational Cost (GFLOPs)', 'GFLOPs'),\n",
    "    ('energy_mwh', axes[1, 1], 'Reds_r', 'Energy Consumption (mWh)', 'Energy (mWh)')\n",
    "]\n",
    "\n",
    "# Plot each metric\n",
    "for metric, ax, palette, title, ylabel in metrics_plot_info:\n",
    "    if metric in benchmark_results:\n",
    "        try:\n",
    "            sns.barplot(x=benchmark_results.index, y=metric, data=benchmark_results, ax=ax, palette=palette)\n",
    "            ax.set_title(title, fontsize=12, weight='bold')\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.tick_params(axis='x', rotation=15)\n",
    "            # Add value labels on bars for accuracy\n",
    "            if metric == 'accuracy':\n",
    "                 for index, value in enumerate(benchmark_results[metric]):\n",
    "                      ax.text(index, value, f'{value:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "            # Handle potential zero values more gracefully in y-limits\n",
    "            if benchmark_results[metric].max() > benchmark_results[metric].min():\n",
    "                 ax.set_ylim(bottom=min(0, benchmark_results[metric].min() * 1.1), top=benchmark_results[metric].max() * 1.1)\n",
    "            elif benchmark_results[metric].max() > 0: # All values same and positive\n",
    "                 ax.set_ylim(bottom = 0, top = benchmark_results[metric].max() * 1.2)\n",
    "            else: # All values are zero or negative\n",
    "                 ax.set_ylim(bottom = benchmark_results[metric].min() * 1.1 if benchmark_results[metric].min() < 0 else -0.1, top = 0.1)\n",
    "\n",
    "        except Exception as e:\n",
    "            builtin_print(f\"Warning: Could not plot {metric}: {e}\")\n",
    "            ax.set_title(f\"{title}\\n(Plotting Error)\", fontsize=12, weight='bold')\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# Pareto Front Plot (Accuracy vs. FLOPs)\n",
    "print(\"\\n--- Generating Pareto Plot (Accuracy vs. FLOPs) ---\")\n",
    "plt.figure(figsize=(10, 7)) # Adjusted size\n",
    "if 'flops_g' in benchmark_results and 'accuracy' in benchmark_results:\n",
    "    try:\n",
    "        sns.scatterplot(\n",
    "            data=benchmark_results,\n",
    "            x='flops_g',\n",
    "            y='accuracy',\n",
    "            hue=benchmark_results.index,\n",
    "            s=150, # Slightly smaller points\n",
    "            style=benchmark_results.index,\n",
    "            palette='viridis'\n",
    "        )\n",
    "        plt.title('Accuracy vs. Computational Cost Trade-off', fontsize=14, weight='bold')\n",
    "        plt.xlabel('Computational Cost (GFLOPs)', fontsize=11)\n",
    "        plt.ylabel('Accuracy', fontsize=11)\n",
    "        plt.legend(title='Model Version', bbox_to_anchor=(1.05, 1), loc='upper left') # Move legend outside\n",
    "        plt.grid(True)\n",
    "        # Adjust x-lim to handle near-zero FLOPs for quantized models better\n",
    "        max_flops = benchmark_results['flops_g'].max()\n",
    "        if max_flops > 0:\n",
    "             plt.xlim(left=-0.001, right=max_flops * 1.1) # Start slightly negative for visibility\n",
    "        else:\n",
    "             plt.xlim(left=-0.001, right=0.001)\n",
    "        # Adjust y-lim based on accuracy range\n",
    "        min_acc = benchmark_results['accuracy'].min()\n",
    "        max_acc = benchmark_results['accuracy'].max()\n",
    "        plt.ylim(bottom=min_acc - 0.01, top=max_acc + 0.01)\n",
    "\n",
    "        plt.tight_layout() # Adjust layout\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Warning: Could not plot Pareto plot: {e}\")\n",
    "\n",
    "else:\n",
    "    builtin_print(\"Skipping Pareto plot: 'flops_g' or 'accuracy' column missing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
