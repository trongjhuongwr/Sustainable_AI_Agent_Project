{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290dfc4e",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "\n",
    "# TRAIN RL AGENT NOTEBOOK\n",
    "\n",
    "    Purpose: Load baseline model & processed data, define RL environment, train PPO agent, save trained agent.\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1a326",
   "metadata": {},
   "source": [
    "# Import Configuration and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c18c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ[\"GYM_DISABLE_WARNINGS\"] = \"true\"\n",
    "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"stable_baselines3\").setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from codecarbon import EmissionsTracker\n",
    "from ptflops import get_model_complexity_info\n",
    "import torch_pruning as tp\n",
    "from tqdm.notebook import tqdm # For agent training progress bar\n",
    "from builtins import print as builtin_print # To avoid conflict with tqdm print\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd3137",
   "metadata": {},
   "source": [
    "# Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths for Loading/Saving\n",
    "    PROCESSED_DATA_PATH = '/kaggle/working/processed_data.pt' # Input path for processed data\n",
    "    BASELINE_MODEL_PATH = '/kaggle/working/baseline_model.pth' # Input path for baseline model\n",
    "    AGENT_SAVE_PATH = \"/kaggle/working/sustainable_ai_agent_ppo.zip\" # Output path for trained agent\n",
    "    BEST_ACTION_SAVE_PATH = \"/kaggle/working/best_action.json\" # Output path for best action info\n",
    "\n",
    "    # Data Parameters (needed for model init and evaluation)\n",
    "    SEQUENCE_LENGTH = 30\n",
    "    INPUT_DIM = 4\n",
    "\n",
    "    # Model Parameters (must match baseline)\n",
    "    HIDDEN_DIM = 256\n",
    "    N_LAYERS = 2\n",
    "    OUTPUT_DIM = 1\n",
    "    DROPOUT = 0.2 # Although dropout is off in eval mode, keep consistent for loading state_dict\n",
    "\n",
    "    # RL Agent Training\n",
    "    TOTAL_TIMESTEPS = 10000\n",
    "    TIMESTEPS_PER_CHUNK = 2000\n",
    "    TENSORBOARD_LOG_PATH = \"/kaggle/working/ppo_tensorboard/\"\n",
    "\n",
    "    # RL Environment Parameters\n",
    "    ACCURACY_PENALTY_THRESHOLD = 0.95 # Accuracy drops below 95% of baseline -> heavy penalty\n",
    "    ACCURACY_REWARD_SCALE = 10\n",
    "    FLOPS_REWARD_SCALE = 1.5\n",
    "    INACTION_PENALTY = -1.0\n",
    "    ENV_ERROR_REWARD = -10.0 # Reward if environment step fails\n",
    "\n",
    "    # Evaluation Parameters (for baseline metrics in env)\n",
    "    EVAL_BATCH_SIZE = 64\n",
    "    CODECARBON_BATCHES = 10 # Number of batches for energy measurement\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running on device: {Config.DEVICE}\")\n",
    "print(f\"Loading processed data from: {Config.PROCESSED_DATA_PATH}\")\n",
    "print(f\"Loading baseline model from: {Config.BASELINE_MODEL_PATH}\")\n",
    "print(f\"Agent will be saved to: {Config.AGENT_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9b627",
   "metadata": {},
   "source": [
    "# Helper Functions and Model Definition\n",
    "\n",
    "# (Copied from baseline notebook, needed for environment and loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Counts trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def evaluate_model(model, loader, codecarbon_batches=10):\n",
    "    \"\"\"Comprehensive evaluation (accuracy, params, flops, energy). Runs on CPU.\"\"\"\n",
    "    model_cpu = copy.deepcopy(model).cpu()\n",
    "    device = torch.device(\"cpu\")\n",
    "    model_cpu.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    # Accuracy\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model_cpu(inputs)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Energy and CO2 with CodeCarbon\n",
    "    energy_kwh = 0\n",
    "    co2_eq_kg = 0\n",
    "    try:\n",
    "        # Save emissions locally in /kaggle/working/\n",
    "        tracker = EmissionsTracker(log_level=\"error\", output_dir=\"/kaggle/working/\", tracking_mode=\"process\")\n",
    "        tracker.start()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, _) in enumerate(loader):\n",
    "                if i >= codecarbon_batches: break # Limit measurement time\n",
    "                model_cpu(inputs.to(device))\n",
    "        tracker.stop()\n",
    "        # Check if emissions data is available (might not be if run time is too short)\n",
    "        if tracker.final_emissions_data:\n",
    "             energy_kwh = tracker.final_emissions_data.energy_consumed or 0\n",
    "             co2_eq_kg = tracker.final_emissions_data.emissions or 0\n",
    "        else:\n",
    "            builtin_print(\"Warning: CodeCarbon tracker did not record final emissions data.\")\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Warning: CodeCarbon measurement failed - {e}\")\n",
    "\n",
    "\n",
    "    # Params\n",
    "    params = count_parameters(model_cpu)\n",
    "\n",
    "    # FLOPs with ptflops\n",
    "    flops = 0\n",
    "    try:\n",
    "        macs, _ = get_model_complexity_info(\n",
    "            model_cpu, (Config.SEQUENCE_LENGTH, Config.INPUT_DIM),\n",
    "            as_strings=False, print_per_layer_stat=False, verbose=False)\n",
    "        flops = macs * 2\n",
    "    except (KeyError, AttributeError, RuntimeError, TypeError): # Added TypeError\n",
    "        # Likely a quantized model or unsupported layer\n",
    "        # builtin_print(\"Note: Could not calculate FLOPs via ptflops (likely quantized model). Reporting 0.\")\n",
    "        flops = 0 # Report 0 to signify significant theoretical reduction\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy, \"energy_kwh\": energy_kwh, \"co2_eq_kg\": co2_eq_kg,\n",
    "        \"flops\": flops, \"params\": params,\n",
    "    }\n",
    "\n",
    "def apply_l1_pruning(model, amount):\n",
    "    \"\"\"Applies L1 pruning, ignoring GRU layers. Returns CPU model.\"\"\"\n",
    "    model_to_prune = copy.deepcopy(model).cpu()\n",
    "    model_to_prune.eval() # Pruning should be done in eval mode for consistency\n",
    "    ignored_layers = [m for m in model_to_prune.modules() if isinstance(m, nn.GRU)]\n",
    "    # Use random input example for pruning analysis\n",
    "    example_inputs = torch.randn(1, Config.SEQUENCE_LENGTH, Config.INPUT_DIM)\n",
    "    pruner = tp.pruner.MagnitudePruner(\n",
    "        model_to_prune,\n",
    "        example_inputs=example_inputs,\n",
    "        importance=tp.importance.MagnitudeImportance(p=1), # L1 norm\n",
    "        pruning_ratio=amount,\n",
    "        ignored_layers=ignored_layers\n",
    "    )\n",
    "    pruner.step()\n",
    "    return model_to_prune\n",
    "\n",
    "def apply_dynamic_quantization(model):\n",
    "    \"\"\"Applies dynamic quantization. Returns CPU model.\"\"\"\n",
    "    quantized_model = copy.deepcopy(model).cpu()\n",
    "    quantized_model.eval() # Quantization requires eval mode\n",
    "    # Quantize GRU and Linear layers\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        quantized_model, {nn.GRU, nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    return quantized_model\n",
    "\n",
    "class WeatherGRU(nn.Module):\n",
    "    \"\"\"GRU model definition (needed to load state_dict).\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim, dropout):\n",
    "        super(WeatherGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers,\n",
    "                          batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "print(\"Helper functions and WeatherGRU class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95223272",
   "metadata": {},
   "source": [
    "# Load Baseline Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeef810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed test data\n",
    "try:\n",
    "    processed_data = torch.load(Config.PROCESSED_DATA_PATH)\n",
    "    X_test_tensor = processed_data['X_test']\n",
    "    y_test_tensor = processed_data['y_test']\n",
    "    print(f\"Processed test data loaded from {Config.PROCESSED_DATA_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data file not found at {Config.PROCESSED_DATA_PATH}. Run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processed data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create Test DataLoader (needed for env initialization)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.EVAL_BATCH_SIZE, shuffle=False)\n",
    "print(\"Test DataLoader created.\")\n",
    "\n",
    "# Initialize and load baseline model\n",
    "baseline_model = WeatherGRU(\n",
    "    input_dim=Config.INPUT_DIM,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    n_layers=Config.N_LAYERS,\n",
    "    output_dim=Config.OUTPUT_DIM,\n",
    "    dropout=Config.DROPOUT # Include dropout for state_dict compatibility\n",
    ")\n",
    "try:\n",
    "    # Load state dict, map location ensures it loads correctly regardless of original device\n",
    "    baseline_model.load_state_dict(torch.load(Config.BASELINE_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "    baseline_model.eval() # Set to evaluation mode\n",
    "    print(f\"Baseline model state loaded from {Config.BASELINE_MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Baseline model file not found at {Config.BASELINE_MODEL_PATH}. Run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading baseline model state: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038bdfa",
   "metadata": {},
   "source": [
    "# Define RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SustainableAIAgentEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for optimizing GRU sustainability.\"\"\"\n",
    "    metadata = {'render_modes': []} # Required for Gymnasium\n",
    "\n",
    "    def __init__(self, model, loader):\n",
    "        super().__init__()\n",
    "        self.base_model = model.cpu() # Ensure base model is on CPU for consistency\n",
    "        self.base_model.eval()\n",
    "        self.loader = loader\n",
    "\n",
    "        builtin_print(\"Calculating baseline metrics for RL environment initialization...\")\n",
    "        # Full evaluation once to get baseline metrics\n",
    "        self.baseline_metrics = evaluate_model(self.base_model, self.loader, Config.CODECARBON_BATCHES)\n",
    "        # Ensure flops is not zero to avoid division errors in reward/state\n",
    "        if self.baseline_metrics.get(\"flops\", 0) == 0:\n",
    "             self.baseline_metrics[\"flops\"] = 1 # Avoid division by zero\n",
    "             builtin_print(\"Warning: Baseline FLOPs calculated as 0. Setting to 1 to avoid division errors.\")\n",
    "        builtin_print(f\"Baseline Metrics initialized: {self.baseline_metrics}\")\n",
    "\n",
    "        # Define action space: [0..7] = pruning only, [8..15] = pruning + quant\n",
    "        self.pruning_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        self.action_space = spaces.Discrete(len(self.pruning_levels) * 2)\n",
    "\n",
    "        # Define observation space: [current_accuracy, acc_vs_baseline, params_reduction, flops_reduction]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0, -1.0, 0.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        terminated = True # Episode ends after one step\n",
    "        truncated = False # Not using truncation\n",
    "        info = {}\n",
    "\n",
    "        try:\n",
    "            # Decode action\n",
    "            action_val = action.item() if isinstance(action, torch.Tensor) else action\n",
    "            pruning_idx = action_val % len(self.pruning_levels)\n",
    "            pruning_amount = self.pruning_levels[pruning_idx]\n",
    "            apply_quant = action_val >= len(self.pruning_levels)\n",
    "\n",
    "            # Apply optimization(s) - these functions return CPU models\n",
    "            optimized_model = copy.deepcopy(self.base_model) # Start fresh\n",
    "            if pruning_amount > 0:\n",
    "                optimized_model = apply_l1_pruning(optimized_model, pruning_amount)\n",
    "            if apply_quant:\n",
    "                optimized_model = apply_dynamic_quantization(optimized_model)\n",
    "            # If action is 0 (no prune, no quant), optimized_model is just the baseline copy\n",
    "\n",
    "            # Lightweight evaluation on CPU\n",
    "            metrics = self._evaluate_lightweight(optimized_model)\n",
    "\n",
    "            # Calculate reward components\n",
    "            accuracy_delta = metrics[\"accuracy\"] - self.baseline_metrics[\"accuracy\"]\n",
    "            if metrics[\"accuracy\"] < (self.baseline_metrics[\"accuracy\"] * Config.ACCURACY_PENALTY_THRESHOLD):\n",
    "                accuracy_reward = Config.ENV_ERROR_REWARD # Heavy penalty if accuracy drops too much\n",
    "            else:\n",
    "                accuracy_reward = accuracy_delta * Config.ACCURACY_REWARD_SCALE\n",
    "\n",
    "            # Ensure baseline flops is not zero before division\n",
    "            baseline_flops = self.baseline_metrics.get(\"flops\", 1) or 1\n",
    "            flops_reduction = max(0.0, 1.0 - (metrics[\"flops\"] / baseline_flops)) # Ensure non-negative\n",
    "            resource_reward = flops_reduction * Config.FLOPS_REWARD_SCALE\n",
    "\n",
    "            inaction_penalty = Config.INACTION_PENALTY if pruning_amount == 0 and not apply_quant else 0.0\n",
    "\n",
    "            reward = accuracy_reward + resource_reward + inaction_penalty\n",
    "\n",
    "            # Calculate state components\n",
    "            baseline_params = self.baseline_metrics.get(\"params\", 1) or 1\n",
    "            params_reduction = max(0.0, 1.0 - (metrics[\"params\"] / baseline_params)) # Ensure non-negative\n",
    "\n",
    "            # Construct observation (next state)\n",
    "            obs = np.array([\n",
    "                metrics[\"accuracy\"],\n",
    "                accuracy_delta,\n",
    "                params_reduction,\n",
    "                flops_reduction\n",
    "            ], dtype=np.float32)\n",
    "            # Clip observation to defined bounds\n",
    "            obs = np.clip(obs, self.observation_space.low, self.observation_space.high)\n",
    "\n",
    "            # Populate info dictionary for logging\n",
    "            info = {\n",
    "                \"action\": action_val,\n",
    "                \"pruning_amount\": pruning_amount,\n",
    "                \"quantized\": apply_quant,\n",
    "                \"accuracy\": metrics[\"accuracy\"],\n",
    "                \"flops_reduction\": flops_reduction,\n",
    "                \"params_reduction\": params_reduction,\n",
    "                \"reward\": reward\n",
    "            }\n",
    "\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        except Exception as e:\n",
    "            builtin_print(f\"Error during env.step with action {action}: {e}\")\n",
    "            # Return initial state, heavy penalty, and terminate\n",
    "            initial_obs, _ = self.reset()\n",
    "            return initial_obs, Config.ENV_ERROR_REWARD, terminated, truncated, {\"error\": str(e)}\n",
    "\n",
    "    def _evaluate_lightweight(self, model):\n",
    "        \"\"\"Quick evaluation on one batch for state/reward. Handles ptflops error.\"\"\"\n",
    "        model.eval()\n",
    "        device = torch.device(\"cpu\") # Lightweight eval always on CPU\n",
    "        metrics = {\"accuracy\": 0.0, \"flops\": self.baseline_metrics.get(\"flops\", 1), \"params\": self.baseline_metrics.get(\"params\", 1)} # Default to baseline\n",
    "\n",
    "        try:\n",
    "            inputs, labels = next(iter(self.loader)) # Get one batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                # Use numpy for accuracy calculation here\n",
    "                accuracy = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "                metrics[\"accuracy\"] = accuracy\n",
    "\n",
    "            # Try to get FLOPs and Params\n",
    "            try:\n",
    "                macs, params = get_model_complexity_info(\n",
    "                    model, (Config.SEQUENCE_LENGTH, Config.INPUT_DIM),\n",
    "                    as_strings=False, print_per_layer_stat=False, verbose=False)\n",
    "                flops = macs * 2\n",
    "                metrics[\"flops\"] = flops if flops > 0 else 0 # Ensure non-negative\n",
    "                metrics[\"params\"] = params if params > 0 else 0\n",
    "            except (RuntimeError, KeyError, AttributeError, TypeError):\n",
    "                 # Failed (likely quantized): Estimate benefits\n",
    "                 metrics[\"flops\"] = 0 # Significant reduction signal\n",
    "                 # Estimate quantized params as 1/4 of baseline float32 params\n",
    "                 metrics[\"params\"] = (self.baseline_metrics.get(\"params\", 1) or 1) / 4\n",
    "\n",
    "        except StopIteration:\n",
    "            builtin_print(\"Warning: DataLoader is empty in _evaluate_lightweight.\")\n",
    "        except Exception as e:\n",
    "            builtin_print(f\"Error during _evaluate_lightweight: {e}\")\n",
    "            # Keep default metrics (baseline) if evaluation fails\n",
    "\n",
    "        # Ensure flops and params are non-negative\n",
    "        metrics[\"flops\"] = max(0.0, metrics[\"flops\"])\n",
    "        metrics[\"params\"] = max(0.0, metrics[\"params\"])\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Initial state: baseline accuracy, 0 delta, 0 reduction\n",
    "        initial_obs = np.array([\n",
    "            self.baseline_metrics.get(\"accuracy\", 0.0),\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0\n",
    "        ], dtype=np.float32)\n",
    "        info = {} # Must return info dict\n",
    "        return initial_obs, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349fc9d",
   "metadata": {},
   "source": [
    "# Initialize and Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44118fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env = SustainableAIAgentEnv(model=baseline_model, loader=test_loader)\n",
    "    # Check if the environment follows the Gymnasium API\n",
    "    # check_env(env) # Can sometimes raise warnings even if env is functional, skip for now\n",
    "    print(\"\\nRL Environment initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError initializing RL Environment: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fafd36",
   "metadata": {},
   "source": [
    "# Train PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize or load Agent\n",
    "if os.path.exists(Config.AGENT_SAVE_PATH):\n",
    "    print(f\"\\n--- Loading pre-trained Agent from: {Config.AGENT_SAVE_PATH} ---\")\n",
    "    try:\n",
    "        agent = PPO.load(Config.AGENT_SAVE_PATH, env=env, device=Config.DEVICE)\n",
    "        print(\"Agent loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading agent: {e}. Creating a new agent.\")\n",
    "        agent = PPO( \"MlpPolicy\", env, verbose=0, device=Config.DEVICE, tensorboard_log=Config.TENSORBOARD_LOG_PATH )\n",
    "        print(\"New PPO Agent created.\")\n",
    "else:\n",
    "    print(f\"\\n--- Agent file not found at {Config.AGENT_SAVE_PATH}. Creating a new PPO Agent. ---\")\n",
    "    agent = PPO( \"MlpPolicy\", env, verbose=0, device=Config.DEVICE, tensorboard_log=Config.TENSORBOARD_LOG_PATH )\n",
    "    print(\"New PPO Agent created.\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "experiment_history = [] # To store results from each step\n",
    "\n",
    "# Callback function to log results\n",
    "def history_callback(local_vars, global_vars):\n",
    "    \"\"\"Callback to store step information.\"\"\"\n",
    "    if 'infos' in local_vars:\n",
    "        for info in local_vars['infos']:\n",
    "            if info and \"action\" in info: # Check if info is not empty and has key data\n",
    "                experiment_history.append(info.copy())\n",
    "    return True # Continue training\n",
    "\n",
    "timesteps_trained_so_far = agent.num_timesteps if hasattr(agent, 'num_timesteps') else 0\n",
    "\n",
    "if timesteps_trained_so_far >= Config.TOTAL_TIMESTEPS:\n",
    "    print(f\"\\n--- Agent already trained for {timesteps_trained_so_far} timesteps. Skipping training. ---\")\n",
    "else:\n",
    "    remaining_timesteps = Config.TOTAL_TIMESTEPS - timesteps_trained_so_far\n",
    "    chunks_to_run = int(np.ceil(remaining_timesteps / Config.TIMESTEPS_PER_CHUNK))\n",
    "\n",
    "    print(f\"\\n--- Starting/Resuming Agent Training ---\")\n",
    "    print(f\"Target: {Config.TOTAL_TIMESTEPS} steps | Already trained: {timesteps_trained_so_far} steps.\")\n",
    "    print(f\"Will run {chunks_to_run} more chunk(s) of {Config.TIMESTEPS_PER_CHUNK} steps each.\")\n",
    "\n",
    "    for i in range(chunks_to_run):\n",
    "        current_chunk = i + 1\n",
    "        steps_this_chunk = min(Config.TIMESTEPS_PER_CHUNK, Config.TOTAL_TIMESTEPS - agent.num_timesteps)\n",
    "        if steps_this_chunk <= 0: break # Should not happen with ceil, but safety check\n",
    "\n",
    "        print(f\"\\n--- Running Chunk {current_chunk}/{chunks_to_run} ({steps_this_chunk} steps) ---\")\n",
    "\n",
    "        try:\n",
    "            agent.learn(\n",
    "                total_timesteps=steps_this_chunk,\n",
    "                reset_num_timesteps=False, # Continue counting total steps\n",
    "                progress_bar=True,\n",
    "                callback=history_callback\n",
    "            )\n",
    "\n",
    "            # Save agent after each chunk\n",
    "            agent.save(Config.AGENT_SAVE_PATH)\n",
    "            current_total_steps = agent.num_timesteps\n",
    "            print(f\"Agent saved. Total timesteps trained: {current_total_steps}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training chunk {current_chunk}: {e}\")\n",
    "            print(\"Attempting to save agent state before exiting...\")\n",
    "            try:\n",
    "                agent.save(Config.AGENT_SAVE_PATH + \"_error\")\n",
    "                print(f\"Agent state saved to {Config.AGENT_SAVE_PATH}_error\")\n",
    "            except Exception as save_e:\n",
    "                print(f\"Could not save agent state after error: {save_e}\")\n",
    "            break # Stop training loop on error\n",
    "\n",
    "        # Check if target reached\n",
    "        if agent.num_timesteps >= Config.TOTAL_TIMESTEPS:\n",
    "            print(f\"\\n--- 🎉 Target of {Config.TOTAL_TIMESTEPS} timesteps reached. ---\")\n",
    "            break\n",
    "\n",
    "print(\"\\n--- Agent Training Loop Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9b2a0",
   "metadata": {},
   "source": [
    "# Analyze Training History and Find Best Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55414fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solution_info_agent = None\n",
    "best_action_agent = None\n",
    "\n",
    "if experiment_history:\n",
    "    history_df = pd.DataFrame(experiment_history)\n",
    "    print(\"\\n--- Agent Experiment History (Sample): ---\")\n",
    "    print(history_df.head().to_markdown(index=False)) # Display first few rows nicely\n",
    "    print(\"...\")\n",
    "    print(history_df.tail().to_markdown(index=False)) # Display last few rows\n",
    "\n",
    "    # Find the step with the highest reward in the history\n",
    "    best_step = history_df.loc[history_df['reward'].idxmax()]\n",
    "\n",
    "    best_solution_info_agent = {\n",
    "        'pruning_amount': best_step['pruning_amount'],\n",
    "        'quantized': best_step['quantized'],\n",
    "        'accuracy_at_step': best_step['accuracy'], # Info from lightweight eval\n",
    "        'reward_at_step': best_step['reward']\n",
    "    }\n",
    "    best_action_agent = int(best_step['action']) # Ensure it's an integer\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BEST SOLUTION FOUND DURING TRAINING (based on reward)\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Action: {best_action_agent}\")\n",
    "    print(f\"Pruning: {best_solution_info_agent['pruning_amount']*100:.0f}%\")\n",
    "    print(f\"Quantized: {best_solution_info_agent['quantized']}\")\n",
    "    print(f\"Achieved Reward: {best_solution_info_agent['reward_at_step']:.4f}\")\n",
    "    print(f\"(Lightweight Eval Accuracy at that step: {best_solution_info_agent['accuracy_at_step']:.4f})\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo experiment history recorded. Predicting best action from final agent policy.\")\n",
    "    try:\n",
    "        obs, _ = env.reset()\n",
    "        action_pred, _ = agent.predict(obs, deterministic=True)\n",
    "        best_action_agent = action_pred.item()\n",
    "\n",
    "        # Decode the predicted action\n",
    "        pruning_idx_pred = best_action_agent % len(env.pruning_levels)\n",
    "        pruning_amount_pred = env.pruning_levels[pruning_idx_pred]\n",
    "        quantized_pred = best_action_agent >= len(env.pruning_levels)\n",
    "\n",
    "        best_solution_info_agent = {\n",
    "            'pruning_amount': pruning_amount_pred,\n",
    "            'quantized': quantized_pred\n",
    "            # No reward/accuracy info available directly from predict\n",
    "        }\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BEST ACTION PREDICTED BY FINAL AGENT POLICY 🤖\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Action: {best_action_agent}\")\n",
    "        print(f\"Pruning: {best_solution_info_agent['pruning_amount']*100:.0f}%\")\n",
    "        print(f\"Quantized: {best_solution_info_agent['quantized']}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting action from final policy: {e}\")\n",
    "        print(\"Cannot determine best action.\")\n",
    "\n",
    "# Save the best action info to JSON (optional but useful for evaluate notebook)\n",
    "if best_solution_info_agent:\n",
    "    try:\n",
    "        with open(Config.BEST_ACTION_SAVE_PATH, 'w') as f:\n",
    "            # Convert numpy types if necessary before saving\n",
    "            serializable_info = {k: (v.item() if isinstance(v, np.generic) else v) for k, v in best_solution_info_agent.items()}\n",
    "            json.dump(serializable_info, f, indent=4)\n",
    "        print(f\"\\nBest action info saved to {Config.BEST_ACTION_SAVE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving best action info: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
