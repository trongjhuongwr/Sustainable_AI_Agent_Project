{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "290dfc4e",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# REINFORCEMENT LEARNING AGENT TRAINING NOTEBOOK\n",
    "# =============================================================================\n",
    "## Purpose:\n",
    "    - Load the pre-trained baseline GRU model and processed dataset.\n",
    "    - Define the custom Reinforcement Learning (RL) environment (`SustainableAIAgentEnv`)\n",
    "      incorporating sustainability objectives (inference FLOPs reduction and training parameter reduction proxy).\n",
    "    - Train a Proximal Policy Optimization (PPO) agent within this environment.\n",
    "    - Save the trained PPO agent and identify the best optimization strategy discovered.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6063a",
   "metadata": {},
   "source": [
    "# === Clone Repository & Install Dependencies ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a08798",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf Sustainable_AI_Agent_Project\n",
    "!git clone https://github.com/trongjhuongwr/Sustainable_AI_Agent_Project.git\n",
    "%cd Sustainable_AI_Agent_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --extra-index-url https://download.pytorch.org/whl/cu121 -r /kaggle/working/Sustainable_AI_Agent_Project/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1a326",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c18c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# Suppress specific warnings for cleaner output\n",
    "os.environ[\"GYM_DISABLE_WARNINGS\"] = \"true\"\n",
    "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"stable_baselines3\").setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from sklearn.metrics import accuracy_score\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from codecarbon import EmissionsTracker\n",
    "from ptflops import get_model_complexity_info\n",
    "import torch_pruning as tp\n",
    "from tqdm.notebook import tqdm\n",
    "from builtins import print as builtin_print\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd3137",
   "metadata": {},
   "source": [
    "# 2. Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines hyperparameters, file paths, and environment parameters used throughout the notebook.\n",
    "class Config:\n",
    "    # --- Input/Output Paths ---\n",
    "    PROCESSED_DATA_PATH = '/kaggle/working/processed_data.pt' # Input: Path to saved processed data tensors\n",
    "    BASELINE_MODEL_PATH = '/kaggle/working/baseline_model.pth' # Input: Path to the trained baseline model state dictionary\n",
    "    AGENT_SAVE_PATH = \"/kaggle/working/sustainable_ai_agent_ppo.zip\" # Output: Path to save/load the trained PPO agent\n",
    "    BEST_ACTION_SAVE_PATH = \"/kaggle/working/best_action.json\" # Output: Path to save information about the best discovered action\n",
    "    TENSORBOARD_LOG_PATH = \"/kaggle/working/ppo_tensorboard/\" # Output: Directory for TensorBoard logs\n",
    "\n",
    "    # --- Data Parameters (consistent with baseline training) ---\n",
    "    SEQUENCE_LENGTH = 30\n",
    "    INPUT_DIM = 4\n",
    "\n",
    "    # --- Model Architecture Parameters (must match baseline) ---\n",
    "    HIDDEN_DIM = 256\n",
    "    N_LAYERS = 2\n",
    "    OUTPUT_DIM = 1\n",
    "    DROPOUT = 0.2 # Required for state_dict compatibility, though inactive in eval mode\n",
    "\n",
    "    # --- RL Agent Training Parameters ---\n",
    "    TOTAL_TIMESTEPS = 10000 # Total number of environment steps for training\n",
    "    TIMESTEPS_PER_CHUNK = 2000 # Save agent state every N steps\n",
    "    SEED = 42 # For reproducibility\n",
    "\n",
    "    # --- RL Environment Parameters (Reward shaping and constraints) ---\n",
    "    ACCURACY_PENALTY_THRESHOLD = 0.95 # Threshold below baseline accuracy triggering heavy penalty (e.g., 0.95 = 5% drop allowed)\n",
    "    ACCURACY_REWARD_SCALE = 10.0 # Scaling factor for accuracy-based reward/penalty\n",
    "    FLOPS_REWARD_SCALE_INFERENCE = 1.5 # Scaling factor for inference FLOPs reduction reward\n",
    "    PARAMS_REWARD_SCALE_TRAINING = 1.0 # NEW: Scaling factor for parameter reduction reward (training energy proxy)\n",
    "    INACTION_PENALTY = -1.0 # Penalty for choosing action 0 (no optimization)\n",
    "    ENV_ERROR_REWARD = -10.0 # Heavy penalty if an environment step fails (e.g., optimization error)\n",
    "\n",
    "    # --- Evaluation Parameters (within the environment) ---\n",
    "    EVAL_BATCH_SIZE = 64 # Batch size used for evaluation within the environment\n",
    "    CODECARBON_BATCHES = 10 # Number of batches used for CodeCarbon energy measurement during env init\n",
    "\n",
    "    # --- Computation Device ---\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Config.SEED)\n",
    "    torch.cuda.manual_seed_all(Config.SEED)\n",
    "    # Optional: Enable deterministic algorithms for full reproducibility, may impact performance\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Configuration loaded. Using device: {Config.DEVICE}\")\n",
    "print(f\"Seed set to: {Config.SEED}\")\n",
    "print(f\"Loading processed data from: {Config.PROCESSED_DATA_PATH}\")\n",
    "print(f\"Loading baseline model from: {Config.BASELINE_MODEL_PATH}\")\n",
    "print(f\"Agent will be saved to: {Config.AGENT_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9b627",
   "metadata": {},
   "source": [
    "# 3. Utility Functions and Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes functions for evaluating model metrics (accuracy, parameters, FLOPs, energy),\n",
    "# applying optimization techniques (pruning, quantization), and the `WeatherGRU` model class\n",
    "# definition (required for loading the baseline state).\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Counts the number of trainable parameters in a PyTorch model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def evaluate_model(model, loader, codecarbon_batches=10):\n",
    "    \"\"\"Performs comprehensive evaluation (accuracy, params, FLOPs, energy).\n",
    "    Crucially, forces execution on CPU for consistent energy measurement.\n",
    "    Args:\n",
    "        model (nn.Module): The model instance to evaluate.\n",
    "        loader (DataLoader): DataLoader for the test/evaluation dataset.\n",
    "        codecarbon_batches (int): Number of batches to run inference on for energy measurement.\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    model_cpu = copy.deepcopy(model).cpu() # Ensure evaluation is on CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    model_cpu.eval() # Set model to evaluation mode\n",
    "\n",
    "    # 1. Accuracy Calculation\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model_cpu(inputs)\n",
    "            preds = (outputs > 0.5).float() # Binary classification threshold\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 2. Energy and CO2 Emission Estimation (using CodeCarbon)\n",
    "    energy_kwh = 0\n",
    "    co2_eq_kg = 0\n",
    "    try:\n",
    "        # Configure tracker for process-level tracking and local output\n",
    "        tracker = EmissionsTracker(log_level=\"error\", output_dir=\"/kaggle/working/\", tracking_mode=\"process\")\n",
    "        tracker.start()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, _) in enumerate(loader):\n",
    "                if i >= codecarbon_batches: break # Limit measurement duration\n",
    "                model_cpu(inputs.to(device))\n",
    "        tracker.stop()\n",
    "        # Safely access emission data, handling cases where tracking might be too short\n",
    "        if tracker.final_emissions_data:\n",
    "             energy_kwh = tracker.final_emissions_data.energy_consumed or 0\n",
    "             co2_eq_kg = tracker.final_emissions_data.emissions or 0\n",
    "        else:\n",
    "            builtin_print(\"Warning: CodeCarbon tracker did not record final emissions data (runtime may be too short).\")\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Warning: CodeCarbon measurement failed - {e}\")\n",
    "\n",
    "    # 3. Parameter Count\n",
    "    params = count_parameters(model_cpu)\n",
    "\n",
    "    # 4. FLOPs Estimation (using ptflops)\n",
    "    flops = 0\n",
    "    try:\n",
    "        # Provide input dimensions for complexity analysis\n",
    "        macs, _ = get_model_complexity_info(\n",
    "            model_cpu, (Config.SEQUENCE_LENGTH, Config.INPUT_DIM),\n",
    "            as_strings=False, print_per_layer_stat=False, verbose=False)\n",
    "        flops = macs * 2 # Approximate FLOPs = 2 * MACs\n",
    "    except (KeyError, AttributeError, RuntimeError, TypeError):\n",
    "        # ptflops often fails with quantized models or certain layer types.\n",
    "        # Report 0 FLOPs as a signal of significant theoretical reduction in these cases.\n",
    "        # builtin_print(\"Note: Could not calculate FLOPs via ptflops (likely quantized model). Reporting 0.\")\n",
    "        flops = 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"energy_kwh\": energy_kwh,\n",
    "        \"co2_eq_kg\": co2_eq_kg,\n",
    "        \"flops\": max(0.0, flops), # Ensure non-negative\n",
    "        \"params\": max(0.0, params) # Ensure non-negative\n",
    "    }\n",
    "\n",
    "def apply_l1_pruning(model, amount):\n",
    "    \"\"\"Applies L1 unstructured magnitude pruning to a model copy.\n",
    "    Excludes GRU layers to preserve recurrent dynamics.\n",
    "    Args:\n",
    "        model (nn.Module): The original model.\n",
    "        amount (float): The target pruning ratio (0.0 to 1.0).\n",
    "    Returns:\n",
    "        nn.Module: A new, pruned model instance on the CPU.\n",
    "    \"\"\"\n",
    "    if not (0 < amount <= 1.0):\n",
    "         # Return a copy if amount is 0 or invalid to avoid errors\n",
    "         return copy.deepcopy(model).cpu()\n",
    "\n",
    "    model_to_prune = copy.deepcopy(model).cpu()\n",
    "    model_to_prune.eval() # Pruning is typically done in evaluation mode\n",
    "\n",
    "    # Identify GRU layers to ignore during pruning\n",
    "    ignored_layers = [m for m in model_to_prune.modules() if isinstance(m, nn.GRU)]\n",
    "\n",
    "    # Create dummy input for dependency graph analysis by the pruner\n",
    "    example_inputs = torch.randn(1, Config.SEQUENCE_LENGTH, Config.INPUT_DIM)\n",
    "\n",
    "    # Use L1 magnitude importance\n",
    "    importance = tp.importance.MagnitudeImportance(p=1)\n",
    "\n",
    "    # Initialize the pruner\n",
    "    pruner = tp.pruner.MagnitudePruner(\n",
    "        model_to_prune,\n",
    "        example_inputs,\n",
    "        importance=importance,\n",
    "        pruning_ratio=amount, # Global pruning ratio\n",
    "        ignored_layers=ignored_layers,\n",
    "    )\n",
    "\n",
    "    # Apply the pruning step (zeros out weights)\n",
    "    try:\n",
    "        pruner.step()\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Error during pruning step with amount {amount}: {e}\")\n",
    "        # Return the unpruned copy in case of error\n",
    "        return copy.deepcopy(model).cpu()\n",
    "\n",
    "    return model_to_prune\n",
    "\n",
    "def apply_dynamic_quantization(model):\n",
    "    \"\"\"Applies PyTorch dynamic quantization (weights to INT8) to relevant layers.\n",
    "    Args:\n",
    "        model (nn.Module): The original model.\n",
    "    Returns:\n",
    "        nn.Module: A new, quantized model instance on the CPU.\n",
    "    \"\"\"\n",
    "    quantized_model = copy.deepcopy(model).cpu()\n",
    "    quantized_model.eval() # Quantization requires evaluation mode\n",
    "\n",
    "    # Specify layers to quantize (GRU and Linear are common targets)\n",
    "    try:\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            quantized_model,\n",
    "            {nn.GRU, nn.Linear}, # Set of layer types to quantize\n",
    "            dtype=torch.qint8 # Target data type\n",
    "        )\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Error during dynamic quantization: {e}\")\n",
    "        # Return the original copy if quantization fails\n",
    "        return copy.deepcopy(model).cpu()\n",
    "\n",
    "    return quantized_model\n",
    "\n",
    "class WeatherGRU(nn.Module):\n",
    "    \"\"\"GRU model definition (replicated from baseline notebook).\n",
    "    Needed here to instantiate the model before loading the state_dict.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim, dropout):\n",
    "        super(WeatherGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers,\n",
    "                          batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        # Use the output from the last time step\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "print(\"Utility functions and WeatherGRU class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95223272",
   "metadata": {},
   "source": [
    "# 4. Load Baseline Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeef810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the necessary artifacts generated by the `train_baseline.ipynb` notebook:\n",
    "# the processed test dataset tensors and the state dictionary of the trained baseline `WeatherGRU` model.\n",
    "\n",
    "# Load processed test data tensors\n",
    "try:\n",
    "    processed_data = torch.load(Config.PROCESSED_DATA_PATH)\n",
    "    X_test_tensor = processed_data['X_test']\n",
    "    y_test_tensor = processed_data['y_test']\n",
    "    builtin_print(f\"Processed test data loaded from {Config.PROCESSED_DATA_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    builtin_print(f\"Error: Processed data file not found at {Config.PROCESSED_DATA_PATH}. Please run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    builtin_print(f\"Error loading processed data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create DataLoader for the test set (used for environment initialization and evaluation)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.EVAL_BATCH_SIZE, shuffle=False)\n",
    "builtin_print(\"Test DataLoader created.\")\n",
    "\n",
    "# Initialize the GRU model structure\n",
    "baseline_model_instance = WeatherGRU(\n",
    "    input_dim=Config.INPUT_DIM,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    n_layers=Config.N_LAYERS,\n",
    "    output_dim=Config.OUTPUT_DIM,\n",
    "    dropout=Config.DROPOUT # Ensure dropout matches the saved model structure\n",
    ")\n",
    "\n",
    "# Load the trained baseline model state dictionary\n",
    "try:\n",
    "    # map_location='cpu' ensures the model loads correctly regardless of the device it was trained on\n",
    "    baseline_model_instance.load_state_dict(torch.load(Config.BASELINE_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "    baseline_model_instance.eval() # Set to evaluation mode by default\n",
    "    builtin_print(f\"Baseline model state loaded from {Config.BASELINE_MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    builtin_print(f\"Error: Baseline model file not found at {Config.BASELINE_MODEL_PATH}. Please run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    builtin_print(f\"Error loading baseline model state: {e}\")\n",
    "    raise\n",
    "\n",
    "# Keep a clean copy on the CPU for the environment\n",
    "baseline_model_cpu = baseline_model_instance.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038bdfa",
   "metadata": {},
   "source": [
    "# 5. Define Custom Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section defines the `SustainableAIAgentEnv` class, inheriting from `gym.Env`.\n",
    "# It encapsulates the optimization problem, defining the action space (pruning ratios, quantization),\n",
    "  # observation space (performance metrics relative to baseline), and the crucial reward function\n",
    "  # that incentivizes both accuracy maintenance and resource reduction (inference FLOPs and training parameters).\n",
    "\n",
    "class SustainableAIAgentEnv(gym.Env):\n",
    "    \"\"\"Custom Gymnasium environment for optimizing GRU model sustainability.\n",
    "\n",
    "    Action Space:\n",
    "        Discrete(16): Corresponds to combinations of L1 pruning ratios [0.0, 0.1, ..., 0.7]\n",
    "                     and the application of dynamic quantization.\n",
    "                     Actions 0-7: Pruning only.\n",
    "                     Actions 8-15: Pruning + Quantization.\n",
    "\n",
    "    Observation Space (Box): [current_accuracy, accuracy_delta_vs_baseline, params_reduction_ratio, flops_reduction_ratio]\n",
    "\n",
    "    Reward Function:\n",
    "        Combines rewards/penalties for:\n",
    "        - Accuracy change relative to baseline (heavy penalty for drops > threshold).\n",
    "        - Reduction in estimated inference FLOPs.\n",
    "        - Reduction in model parameters (proxy for training energy reduction).\n",
    "        - Inaction (choosing 0% pruning and no quantization).\n",
    "    \"\"\"\n",
    "    metadata = {'render_modes': []} # Required by Gymnasium API\n",
    "\n",
    "    def __init__(self, baseline_model, data_loader):\n",
    "        \"\"\"Initializes the environment.\n",
    "        Args:\n",
    "            baseline_model (nn.Module): The pre-trained baseline model instance (on CPU).\n",
    "            data_loader (DataLoader): DataLoader for evaluation (typically test set).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_model = baseline_model.cpu() # Ensure base model is on CPU\n",
    "        self.base_model.eval()\n",
    "        self.loader = data_loader\n",
    "\n",
    "        builtin_print(\"Initializing RL environment: Calculating baseline metrics...\")\n",
    "        # Perform a full evaluation once at initialization to get baseline reference metrics\n",
    "        self.baseline_metrics = evaluate_model(self.base_model, self.loader, Config.CODECARBON_BATCHES)\n",
    "\n",
    "        # Handle potential division by zero if baseline FLOPs/params are 0 (e.g., from ptflops error)\n",
    "        if self.baseline_metrics.get(\"flops\", 0) == 0:\n",
    "            self.baseline_metrics[\"flops\"] = 1 # Avoid division by zero\n",
    "            builtin_print(\"Warning: Baseline FLOPs calculated as 0. Setting to 1 to prevent division errors.\")\n",
    "        if self.baseline_metrics.get(\"params\", 0) == 0:\n",
    "             self.baseline_metrics[\"params\"] = 1 # Avoid division by zero\n",
    "             builtin_print(\"Warning: Baseline Parameters calculated as 0. Setting to 1 to prevent division errors.\")\n",
    "\n",
    "        builtin_print(f\"Baseline Metrics initialized: {self.baseline_metrics}\")\n",
    "\n",
    "        # --- Action Space Definition ---\n",
    "        self.pruning_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        n_pruning_levels = len(self.pruning_levels)\n",
    "        self.action_space = spaces.Discrete(n_pruning_levels * 2) # 8 pruning levels * (prune_only + prune_and_quantize)\n",
    "\n",
    "        # --- Observation Space Definition ---\n",
    "        # [current_accuracy, accuracy_delta, params_reduction_ratio, flops_reduction_ratio]\n",
    "        # Bounds ensure values stay within a reasonable range (e.g., accuracy 0-1, reduction 0-1, delta -1 to 1)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0, -1.0, 0.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Store initial observation for reset\n",
    "        self.initial_obs = np.array([\n",
    "            self.baseline_metrics.get(\"accuracy\", 0.0),\n",
    "            0.0, # Initial accuracy delta\n",
    "            0.0, # Initial params reduction\n",
    "            0.0  # Initial FLOPs reduction\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Executes one step in the environment based on the agent's action.\n",
    "        Args:\n",
    "            action (int): The discrete action selected by the agent.\n",
    "        Returns:\n",
    "            tuple: (observation, reward, terminated, truncated, info)\n",
    "        \"\"\"\n",
    "        terminated = True # This environment is episodic with a single step\n",
    "        truncated = False # Not using truncation\n",
    "        info = {} # Dictionary for logging diagnostic information\n",
    "\n",
    "        try:\n",
    "            # --- 1. Decode Action ---\n",
    "            action_val = action.item() if isinstance(action, torch.Tensor) else action\n",
    "            n_pruning_levels = len(self.pruning_levels)\n",
    "            pruning_idx = action_val % n_pruning_levels\n",
    "            pruning_amount = self.pruning_levels[pruning_idx]\n",
    "            apply_quant = action_val >= n_pruning_levels\n",
    "\n",
    "            # --- 2. Apply Optimization(s) ---\n",
    "            # Start with a fresh copy of the baseline model (on CPU)\n",
    "            optimized_model = copy.deepcopy(self.base_model)\n",
    "            if pruning_amount > 0:\n",
    "                optimized_model = apply_l1_pruning(optimized_model, pruning_amount)\n",
    "            if apply_quant:\n",
    "                optimized_model = apply_dynamic_quantization(optimized_model)\n",
    "            # If action is 0 (no prune, no quant), optimized_model remains a copy of the baseline\n",
    "\n",
    "            # --- 3. Lightweight Evaluation ---\n",
    "            # Evaluate the optimized model quickly (e.g., on one batch) to get metrics for reward/state\n",
    "            metrics = self._evaluate_lightweight(optimized_model)\n",
    "\n",
    "            # --- 4. Calculate Reward Components (MODIFIED FOR DIRECTION 1) ---\n",
    "            baseline_acc = self.baseline_metrics.get(\"accuracy\", 0.0)\n",
    "            current_acc = metrics.get(\"accuracy\", 0.0)\n",
    "            accuracy_delta = current_acc - baseline_acc\n",
    "\n",
    "            # a) Accuracy Reward/Penalty\n",
    "            if current_acc < (baseline_acc * Config.ACCURACY_PENALTY_THRESHOLD):\n",
    "                accuracy_reward = Config.ENV_ERROR_REWARD # Heavy penalty for significant accuracy drop\n",
    "            else:\n",
    "                accuracy_reward = accuracy_delta * Config.ACCURACY_REWARD_SCALE # Proportional reward/penalty\n",
    "\n",
    "            # b) Inference Resource Reward (FLOPs Reduction)\n",
    "            baseline_flops = self.baseline_metrics.get(\"flops\", 1) # Default to 1 if missing\n",
    "            current_flops = metrics.get(\"flops\", baseline_flops)\n",
    "            flops_reduction_ratio = max(0.0, 1.0 - (current_flops / baseline_flops)) # Ensure non-negative\n",
    "            resource_reward_inference = flops_reduction_ratio * Config.FLOPS_REWARD_SCALE_INFERENCE\n",
    "\n",
    "            # c) Training Resource Reward Proxy (Parameter Reduction) - NEW\n",
    "            baseline_params = self.baseline_metrics.get(\"params\", 1) # Default to 1 if missing\n",
    "            current_params = metrics.get(\"params\", baseline_params)\n",
    "            params_reduction_ratio = max(0.0, 1.0 - (current_params / baseline_params)) # Ensure non-negative\n",
    "            resource_reward_training = params_reduction_ratio * Config.PARAMS_REWARD_SCALE_TRAINING\n",
    "\n",
    "            # d) Inaction Penalty\n",
    "            inaction_penalty = Config.INACTION_PENALTY if pruning_amount == 0 and not apply_quant else 0.0\n",
    "\n",
    "            # e) Total Reward\n",
    "            reward = accuracy_reward + resource_reward_inference + resource_reward_training + inaction_penalty\n",
    "\n",
    "            # --- 5. Construct Observation (Next State) ---\n",
    "            obs = np.array([\n",
    "                current_acc,\n",
    "                accuracy_delta,\n",
    "                params_reduction_ratio,\n",
    "                flops_reduction_ratio\n",
    "            ], dtype=np.float32)\n",
    "\n",
    "            # Clip observation to ensure it's within the defined space bounds\n",
    "            obs = np.clip(obs, self.observation_space.low, self.observation_space.high)\n",
    "\n",
    "            # --- 6. Populate Info Dictionary --- (Added reward components)\n",
    "            info = {\n",
    "                \"action\": action_val,\n",
    "                \"pruning_amount\": pruning_amount,\n",
    "                \"quantized\": apply_quant,\n",
    "                \"accuracy\": current_acc,\n",
    "                \"accuracy_delta\": accuracy_delta,\n",
    "                \"flops_reduction\": flops_reduction_ratio,\n",
    "                \"params_reduction\": params_reduction_ratio,\n",
    "                \"reward_components\": {\n",
    "                    \"accuracy\": accuracy_reward,\n",
    "                    \"flops_inference\": resource_reward_inference,\n",
    "                    \"params_training\": resource_reward_training,\n",
    "                    \"inaction\": inaction_penalty\n",
    "                },\n",
    "                \"reward\": reward\n",
    "            }\n",
    "\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        except Exception as e:\n",
    "            builtin_print(f\"Error during environment step with action {action}: {e}\")\n",
    "            # Return initial state, heavy penalty, and terminate episode on error\n",
    "            obs, _ = self.reset()\n",
    "            return obs, Config.ENV_ERROR_REWARD, True, False, {\"error\": str(e)}\n",
    "\n",
    "    def _evaluate_lightweight(self, model):\n",
    "        \"\"\"Performs a quick evaluation on a single batch for state/reward calculation.\n",
    "        Handles potential ptflops errors for quantized models by estimating reductions.\n",
    "        Always runs on CPU.\n",
    "        Args:\n",
    "            model (nn.Module): The optimized model (on CPU).\n",
    "        Returns:\n",
    "            dict: Dictionary with 'accuracy', 'flops', 'params'.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        device = torch.device(\"cpu\") # Lightweight eval always on CPU\n",
    "        # Initialize metrics with baseline defaults in case of evaluation error\n",
    "        metrics = {\n",
    "            \"accuracy\": self.baseline_metrics.get(\"accuracy\", 0.0),\n",
    "            \"flops\": self.baseline_metrics.get(\"flops\", 1),\n",
    "            \"params\": self.baseline_metrics.get(\"params\", 1)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            inputs, labels = next(iter(self.loader)) # Get a single batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Calculate accuracy on the batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                # Use numpy for accuracy calculation here\n",
    "                accuracy = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "                metrics[\"accuracy\"] = accuracy\n",
    "\n",
    "            # Attempt to calculate FLOPs and Params\n",
    "            try:\n",
    "                macs, params_val = get_model_complexity_info(\n",
    "                    model, (Config.SEQUENCE_LENGTH, Config.INPUT_DIM),\n",
    "                    as_strings=False, print_per_layer_stat=False, verbose=False)\n",
    "                flops_val = macs * 2\n",
    "                metrics[\"flops\"] = flops_val\n",
    "                metrics[\"params\"] = params_val\n",
    "            except (RuntimeError, KeyError, AttributeError, TypeError):\n",
    "                 # ptflops failed (likely due to quantization)\n",
    "                 # Estimate benefits:\n",
    "                 metrics[\"flops\"] = 0 # Signal significant theoretical reduction\n",
    "                 # Estimate quantized params roughly as 1/4 of baseline float32 params\n",
    "                 metrics[\"params\"] = self.baseline_metrics.get(\"params\", 1) / 4\n",
    "\n",
    "        except StopIteration:\n",
    "            builtin_print(\"Warning: DataLoader is empty in _evaluate_lightweight.\")\n",
    "        except Exception as e:\n",
    "            builtin_print(f\"Error during _evaluate_lightweight: {e}\")\n",
    "            # Keep default metrics (baseline) if evaluation fails\n",
    "\n",
    "        # Ensure reported metrics are non-negative\n",
    "        metrics[\"flops\"] = max(0.0, metrics[\"flops\"])\n",
    "        metrics[\"params\"] = max(0.0, metrics[\"params\"])\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to the initial state.\n",
    "        Args:\n",
    "            seed (int, optional): Seed for the random number generator.\n",
    "            options (dict, optional): Additional options (not used here).\n",
    "        Returns:\n",
    "            tuple: (initial_observation, info)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        # Return the pre-calculated initial observation corresponding to the baseline model\n",
    "        info = {} # Must return an info dictionary\n",
    "        return self.initial_obs, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the environment (not applicable here).\"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the environment (cleanup, not applicable here).\"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"SustainableAIAgentEnv class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349fc9d",
   "metadata": {},
   "source": [
    "# 6. Initialize and Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44118fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates the custom environment and performs a check using\n",
    "# `stable_baselines3.common.env_checker` to ensure API compatibility.\n",
    "\n",
    "try:\n",
    "    # Instantiate the environment with the baseline model and test loader\n",
    "    env = SustainableAIAgentEnv(model=baseline_model_cpu, loader=test_loader)\n",
    "\n",
    "    # Optional: Verify the environment conforms to the Gymnasium API\n",
    "    # check_env(env) # This can sometimes raise warnings even if functional; skip if causing issues\n",
    "    builtin_print(\"\\nRL Environment initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    builtin_print(f\"\\nError initializing RL Environment: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fafd36",
   "metadata": {},
   "source": [
    "# 7. Train PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes or loads a PPO agent from `stable_baselines3` and trains it using the custom environment.\n",
    "# Training proceeds in chunks, saving the agent's state periodically.\n",
    "# A custom callback is used to log detailed information about each optimization attempt during training.\n",
    "\n",
    "# --- Agent Initialization or Loading ---\n",
    "if os.path.exists(Config.AGENT_SAVE_PATH):\n",
    "    builtin_print(f\"\\n--- Loading pre-trained Agent from: {Config.AGENT_SAVE_PATH} ---\")\n",
    "    try:\n",
    "        agent = PPO.load(Config.AGENT_SAVE_PATH, env=env, device=Config.DEVICE)\n",
    "        builtin_print(\"Agent loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Error loading agent: {e}. Creating a new agent.\")\n",
    "        agent = PPO(\"MlpPolicy\", env, verbose=0, device=Config.DEVICE,\n",
    "                    tensorboard_log=Config.TENSORBOARD_LOG_PATH, seed=Config.SEED)\n",
    "        builtin_print(\"New PPO Agent created.\")\n",
    "else:\n",
    "    builtin_print(f\"\\n--- Agent file not found at {Config.AGENT_SAVE_PATH}. Creating a new PPO Agent. ---\")\n",
    "    agent = PPO(\"MlpPolicy\", env, verbose=0, device=Config.DEVICE,\n",
    "                tensorboard_log=Config.TENSORBOARD_LOG_PATH, seed=Config.SEED)\n",
    "    builtin_print(\"New PPO Agent created.\")\n",
    "\n",
    "# --- Custom Callback for History Logging ---\n",
    "experiment_history = [] # List to store info dictionaries from each step\n",
    "\n",
    "class HistoryCallback(BaseCallback):\n",
    "    \"\"\"A custom callback to store the info dictionary returned by env.step().\"\"\"\n",
    "    def _on_step(self) -> bool:\n",
    "        # infos is a list of info dicts, one for each env (here, only one)\n",
    "        if 'infos' in self.locals:\n",
    "            for info in self.locals['infos']:\n",
    "                # Ensure info is not empty and contains expected keys before logging\n",
    "                if info and \"action\" in info and \"reward\" in info:\n",
    "                    experiment_history.append(info.copy()) # Store a copy\n",
    "        return True # Return True to continue training\n",
    "\n",
    "history_callback = HistoryCallback()\n",
    "\n",
    "# --- Training Loop (Chunk-based with saving) ---\n",
    "timesteps_trained_so_far = agent.num_timesteps if hasattr(agent, 'num_timesteps') else 0\n",
    "\n",
    "if timesteps_trained_so_far >= Config.TOTAL_TIMESTEPS:\n",
    "    builtin_print(f\"\\n--- Agent already trained for {timesteps_trained_so_far} timesteps. Skipping training. ---\")\n",
    "else:\n",
    "    remaining_timesteps = Config.TOTAL_TIMESTEPS - timesteps_trained_so_far\n",
    "    # Calculate number of chunks needed, rounding up\n",
    "    chunks_to_run = int(np.ceil(remaining_timesteps / Config.TIMESTEPS_PER_CHUNK))\n",
    "\n",
    "    builtin_print(f\"\\n--- Starting/Resuming Agent Training ---\")\n",
    "    builtin_print(f\"Target Timesteps: {Config.TOTAL_TIMESTEPS} | Current Timesteps: {timesteps_trained_so_far}\")\n",
    "    builtin_print(f\"Training will proceed in {chunks_to_run} chunk(s) of up to {Config.TIMESTEPS_PER_CHUNK} steps each.\")\n",
    "\n",
    "    for i in range(chunks_to_run):\n",
    "        current_chunk_number = i + 1\n",
    "        # Calculate steps for this specific chunk, ensuring not to exceed total timesteps\n",
    "        steps_this_chunk = min(Config.TIMESTEPS_PER_CHUNK, Config.TOTAL_TIMESTEPS - agent.num_timesteps)\n",
    "\n",
    "        if steps_this_chunk <= 0: break # Safety check\n",
    "\n",
    "        builtin_print(f\"\\n--- Running Training Chunk {current_chunk_number}/{chunks_to_run} ({steps_this_chunk} steps) ---\")\n",
    "\n",
    "        try:\n",
    "            # Execute the learning process for the calculated number of steps\n",
    "            agent.learn(\n",
    "                total_timesteps=steps_this_chunk,\n",
    "                reset_num_timesteps=False, # Continue timestep count across chunks\n",
    "                progress_bar=True, # Display tqdm progress bar\n",
    "                callback=history_callback # Log step info using the custom callback\n",
    "            )\n",
    "\n",
    "            # Save the agent's state after completing the chunk\n",
    "            agent.save(Config.AGENT_SAVE_PATH)\n",
    "            current_total_steps = agent.num_timesteps\n",
    "            builtin_print(f\"Agent state saved to {Config.AGENT_SAVE_PATH}. Total timesteps trained: {current_total_steps}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            builtin_print(f\"Error occurred during training chunk {current_chunk_number}: {e}\")\n",
    "            builtin_print(\"Attempting to save agent state before exiting...\")\n",
    "            # Try saving an error state for potential recovery\n",
    "            try:\n",
    "                agent.save(Config.AGENT_SAVE_PATH + \"_error\")\n",
    "                builtin_print(f\"Agent error state saved to {Config.AGENT_SAVE_PATH}_error\")\n",
    "            except Exception as save_e:\n",
    "                builtin_print(f\"Could not save agent state after error: {save_e}\")\n",
    "            break # Stop the training loop if an error occurs\n",
    "\n",
    "        # Check if the total timesteps target has been reached\n",
    "        if agent.num_timesteps >= Config.TOTAL_TIMESTEPS:\n",
    "            builtin_print(f\"\\n--- ðŸŽ‰ Target of {Config.TOTAL_TIMESTEPS} timesteps reached. Finishing training. ---\")\n",
    "            break\n",
    "\n",
    "builtin_print(\"\\n--- Agent Training Loop Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9b2a0",
   "metadata": {},
   "source": [
    "# 8. Analyze Training History and Determine Best Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55414fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examines the `experiment_history` logged by the callback to find the optimization action\n",
    "# (pruning ratio + quantization) that yielded the highest reward during training.\n",
    "# If no history is available (e.g., agent loaded after training), it predicts the best action\n",
    "# based on the agent's final learned policy.\n",
    "# The best action information is saved to a JSON file for potential use in the evaluation notebook.\n",
    "\n",
    "best_action_info = None\n",
    "best_action_code = None\n",
    "\n",
    "# --- Analysis based on logged history (preferred) ---\n",
    "if experiment_history:\n",
    "    # Convert history list of dicts to a Pandas DataFrame for easier analysis\n",
    "    history_df = pd.DataFrame(experiment_history)\n",
    "    builtin_print(\"\\n--- Agent Training Experiment History (Sample) ---\")\n",
    "    # Display head and tail for a quick overview\n",
    "    builtin_print(\"First 5 steps:\")\n",
    "    builtin_print(history_df.head().to_markdown(index=False))\n",
    "    builtin_print(\"\\nLast 5 steps:\")\n",
    "    builtin_print(history_df.tail().to_markdown(index=False))\n",
    "\n",
    "    # Find the row (step) corresponding to the maximum reward achieved\n",
    "    if not history_df.empty and 'reward' in history_df.columns:\n",
    "        best_step_index = history_df['reward'].idxmax()\n",
    "        best_step_data = history_df.loc[best_step_index]\n",
    "\n",
    "        # Extract relevant information from the best step\n",
    "        best_action_info = {\n",
    "            'source': 'history_max_reward',\n",
    "            'step_index': int(best_step_index), # Ensure index is standard int\n",
    "            'pruning_amount': best_step_data.get('pruning_amount', 0.0),\n",
    "            'quantized': best_step_data.get('quantized', False),\n",
    "            'reward_at_step': best_step_data.get('reward', None),\n",
    "            'accuracy_at_step': best_step_data.get('accuracy', None), # From lightweight eval\n",
    "            'flops_reduct_at_step': best_step_data.get('flops_reduction', None),\n",
    "            'params_reduct_at_step': best_step_data.get('params_reduction', None),\n",
    "            # Optionally include reward components if logged\n",
    "            'reward_components_at_step': best_step_data.get('reward_components', None)\n",
    "        }\n",
    "        best_action_code = int(best_step_data.get('action', -1)) # Get the action code as int\n",
    "\n",
    "        builtin_print(\"\\n\" + \"=\"*60)\n",
    "        builtin_print(\"BEST OPTIMIZATION STRATEGY FOUND DURING TRAINING (Max Reward)\")\n",
    "        builtin_print(\"=\"*60)\n",
    "        builtin_print(f\"Action Code: {best_action_code}\")\n",
    "        builtin_print(f\"Pruning Ratio: {best_action_info['pruning_amount']*100:.0f}%\")\n",
    "        builtin_print(f\"Quantization Applied: {best_action_info['quantized']}\")\n",
    "        builtin_print(f\"Achieved Reward: {best_action_info['reward_at_step']:.4f}\")\n",
    "        builtin_print(f\"  (Accuracy at step [lightweight eval]: {best_action_info['accuracy_at_step']:.4f})\")\n",
    "        builtin_print(f\"  (FLOPs Reduction at step: {best_action_info['flops_reduct_at_step']:.4f})\")\n",
    "        builtin_print(f\"  (Params Reduction at step: {best_action_info['params_reduct_at_step']:.4f})\")\n",
    "        builtin_print(\"=\"*60)\n",
    "    else:\n",
    "        builtin_print(\"\\nWarning: Experiment history is empty or lacks 'reward' column. Cannot determine best action from history.\")\n",
    "\n",
    "# --- Fallback: Predict action from the final agent policy ---\n",
    "if best_action_info is None:\n",
    "    builtin_print(\"\\n--- Predicting best action from the final trained agent policy ---\")\n",
    "    try:\n",
    "        # Ensure the environment is reset to get the initial observation\n",
    "        obs, _ = env.reset()\n",
    "        # Predict the action deterministically based on the learned policy\n",
    "        action_pred, _ = agent.predict(obs, deterministic=True)\n",
    "        best_action_code = action_pred.item() # Get the action code as int\n",
    "\n",
    "        # Decode the predicted action to get pruning amount and quantization status\n",
    "        n_pruning_levels = len(env.pruning_levels)\n",
    "        pruning_idx_pred = best_action_code % n_pruning_levels\n",
    "        pruning_amount_pred = env.pruning_levels[pruning_idx_pred]\n",
    "        quantized_pred = best_action_code >= n_pruning_levels\n",
    "\n",
    "        # Store the predicted action information\n",
    "        best_action_info = {\n",
    "            'source': 'final_policy_prediction',\n",
    "            'pruning_amount': pruning_amount_pred,\n",
    "            'quantized': quantized_pred\n",
    "            # Note: Reward and metrics are not directly available from predict\n",
    "        }\n",
    "\n",
    "        builtin_print(\"\\n\" + \"=\"*60)\n",
    "        builtin_print(\"BEST ACTION PREDICTED BY FINAL AGENT POLICY\")\n",
    "        builtin_print(\"=\"*60)\n",
    "        builtin_print(f\"Action Code: {best_action_code}\")\n",
    "        builtin_print(f\"Pruning Ratio: {best_action_info['pruning_amount']*100:.0f}%\")\n",
    "        builtin_print(f\"Quantization Applied: {best_action_info['quantized']}\")\n",
    "        builtin_print(\"=\"*60)\n",
    "\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"\\nError predicting action from final policy: {e}\")\n",
    "        builtin_print(\"Could not determine the best action.\")\n",
    "\n",
    "# --- Save the best action information to a JSON file ---\n",
    "# This allows the evaluation notebook to directly use the best strategy without reloading the agent\n",
    "if best_action_info:\n",
    "    try:\n",
    "        # Ensure data types are JSON serializable (convert numpy types if present)\n",
    "        serializable_info = {k: (v.item() if isinstance(v, (np.generic, np.ndarray)) else v) for k, v in best_action_info.items()}\n",
    "\n",
    "        # Add the action code itself to the saved info\n",
    "        if best_action_code is not None:\n",
    "            serializable_info['action_code'] = best_action_code\n",
    "\n",
    "        with open(Config.BEST_ACTION_SAVE_PATH, 'w') as f:\n",
    "            json.dump(serializable_info, f, indent=4)\n",
    "        builtin_print(f\"\\nBest action information saved to {Config.BEST_ACTION_SAVE_PATH}\")\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"\\nError saving best action information to JSON: {e}\")\n",
    "else:\n",
    "    builtin_print(\"\\nNo best action information was determined, skipping save to JSON.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
