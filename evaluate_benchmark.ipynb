{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f3e735",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK EVALUATION NOTEBOOK\n",
    "# =============================================================================\n",
    "## Purpose:\n",
    "    - Load the baseline model, processed test data, and the best action determined by the RL agent.\n",
    "    - Apply the baseline configuration, manual optimization strategies (pruning, quantization), and the agent-discovered optimization strategy to the baseline model.\n",
    "    - Perform a comprehensive benchmark evaluation of all model versions using standardized metrics (accuracy, parameters, FLOPs, energy consumption).\n",
    "    - Visualize the comparative performance results.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5d74a",
   "metadata": {},
   "source": [
    "# === Clone Repository & Install Dependencies ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf Sustainable_AI_Agent_Project\n",
    "!git clone https://github.com/trongjhuongwr/Sustainable_AI_Agent_Project.git\n",
    "%cd Sustainable_AI_Agent_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ed504",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --extra-index-url https://download.pytorch.org/whl/cu121 -r /kaggle/working/Sustainable_AI_Agent_Project/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b880f",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress specific warnings for cleaner output\n",
    "os.environ[\"GYM_DISABLE_WARNINGS\"] = \"true\"\n",
    "warnings.filterwarnings(\"ignore\", module=\"gymnasium\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"stable_baselines3\").setLevel(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym # Required for potential agent loading fallback\n",
    "from gymnasium import spaces # Required for potential agent loading fallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "from stable_baselines3 import PPO # Required for potential agent loading fallback\n",
    "from codecarbon import EmissionsTracker\n",
    "from ptflops import get_model_complexity_info\n",
    "import torch_pruning as tp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from builtins import print as builtin_print # Avoid tqdm print conflicts\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2120cde",
   "metadata": {},
   "source": [
    "# 2. Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines hyperparameters, file paths, and evaluation parameters.\n",
    "class Config:\n",
    "    # --- Input Paths ---\n",
    "    PROCESSED_DATA_PATH = '/kaggle/working/processed_data.pt' # Path to processed data tensors\n",
    "    BASELINE_MODEL_PATH = '/kaggle/working/baseline_model.pth' # Path to baseline model state dictionary\n",
    "    AGENT_SAVE_PATH = \"/kaggle/working/sustainable_ai_agent_ppo.zip\" # Path to saved PPO agent (fallback)\n",
    "    BEST_ACTION_PATH = \"/kaggle/working/best_action.json\" # Preferred path for best action info\n",
    "\n",
    "    # --- Data Parameters ---\n",
    "    SEQUENCE_LENGTH = 30\n",
    "    INPUT_DIM = 4\n",
    "\n",
    "    # --- Model Architecture Parameters (must match baseline) ---\n",
    "    HIDDEN_DIM = 256\n",
    "    N_LAYERS = 2\n",
    "    OUTPUT_DIM = 1\n",
    "    DROPOUT = 0.2\n",
    "    SEED = 42 # For reproducibility if agent needs loading\n",
    "\n",
    "    # --- Evaluation Parameters ---\n",
    "    EVAL_BATCH_SIZE = 64\n",
    "    CODECARBON_BATCHES = 10 # Batches for energy measurement\n",
    "\n",
    "    # --- Computation Device ---\n",
    "    # Evaluation typically runs on CPU for consistent energy metrics,\n",
    "    # but DEVICE might be needed if loading the agent requires GPU context.\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds (primarily relevant if agent needs to be loaded and predict)\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(Config.SEED)\n",
    "    torch.cuda.manual_seed_all(Config.SEED)\n",
    "\n",
    "print(f\"Configuration loaded. Evaluation device preference: CPU for metrics.\")\n",
    "print(f\"Loading data from: {Config.PROCESSED_DATA_PATH}\")\n",
    "print(f\"Loading baseline model from: {Config.BASELINE_MODEL_PATH}\")\n",
    "print(f\"Attempting to load best action from: {Config.BEST_ACTION_PATH}\")\n",
    "print(f\"Agent path (fallback): {Config.AGENT_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa52a2",
   "metadata": {},
   "source": [
    "# 3. Utility Functions and Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicated from previous notebooks for standalone execution capability.\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Counts trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def evaluate_model(model, loader, codecarbon_batches=10):\n",
    "    \"\"\"Comprehensive evaluation (accuracy, params, flops, energy). Forces CPU execution.\"\"\"\n",
    "    model_cpu = copy.deepcopy(model).cpu()\n",
    "    device = torch.device(\"cpu\")\n",
    "    model_cpu.eval()\n",
    "\n",
    "    # Accuracy\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model_cpu(inputs)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Energy and CO2 with CodeCarbon\n",
    "    energy_kwh = 0\n",
    "    co2_eq_kg = 0\n",
    "    try:\n",
    "        tracker = EmissionsTracker(log_level=\"error\", output_dir=\"/kaggle/working/\", tracking_mode=\"process\")\n",
    "        tracker.start()\n",
    "        with torch.no_grad():\n",
    "            # Use tqdm for energy measurement loop visualization\n",
    "            energy_eval_bar = tqdm(loader, desc=\"Measuring Energy\", total=codecarbon_batches, leave=False)\n",
    "            batch_count = 0\n",
    "            for inputs, _ in energy_eval_bar:\n",
    "                if batch_count >= codecarbon_batches: break\n",
    "                model_cpu(inputs.to(device))\n",
    "                batch_count += 1\n",
    "        tracker.stop()\n",
    "        if tracker.final_emissions_data:\n",
    "             energy_kwh = tracker.final_emissions_data.energy_consumed or 0\n",
    "             co2_eq_kg = tracker.final_emissions_data.emissions or 0\n",
    "        else:\n",
    "            builtin_print(\"Warning: CodeCarbon tracker did not record final emissions data.\")\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Warning: CodeCarbon measurement failed - {e}\")\n",
    "\n",
    "    # Params\n",
    "    params = count_parameters(model_cpu)\n",
    "\n",
    "    # FLOPs with ptflops\n",
    "    flops = 0\n",
    "    try:\n",
    "        macs, _ = get_model_complexity_info(\n",
    "            model_cpu, (Config.SEQUENCE_LENGTH, Config.INPUT_DIM),\n",
    "            as_strings=False, print_per_layer_stat=False, verbose=False)\n",
    "        flops = macs * 2\n",
    "    except (KeyError, AttributeError, RuntimeError, TypeError):\n",
    "        # builtin_print(\"Note: Could not calculate FLOPs via ptflops (likely quantized model). Reporting 0.\")\n",
    "        flops = 0 # Report 0 for quantized models\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"energy_kwh\": energy_kwh,\n",
    "        \"co2_eq_kg\": co2_eq_kg,\n",
    "        \"flops\": max(0.0, flops),\n",
    "        \"params\": max(0.0, params)\n",
    "    }\n",
    "\n",
    "def apply_l1_pruning(model, amount):\n",
    "    \"\"\"Applies L1 pruning, ignoring GRU layers. Returns CPU model.\"\"\"\n",
    "    if not (0 < amount <= 1.0): return copy.deepcopy(model).cpu()\n",
    "    model_to_prune = copy.deepcopy(model).cpu()\n",
    "    model_to_prune.eval()\n",
    "    ignored_layers = [m for m in model_to_prune.modules() if isinstance(m, nn.GRU)]\n",
    "    example_inputs = torch.randn(1, Config.SEQUENCE_LENGTH, Config.INPUT_DIM)\n",
    "    try:\n",
    "        pruner = tp.pruner.MagnitudePruner(\n",
    "            model_to_prune, example_inputs,\n",
    "            importance=tp.importance.MagnitudeImportance(p=1),\n",
    "            pruning_ratio=amount, ignored_layers=ignored_layers)\n",
    "        pruner.step()\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Error during pruning: {e}\")\n",
    "        return copy.deepcopy(model).cpu()\n",
    "    return model_to_prune\n",
    "\n",
    "def apply_dynamic_quantization(model):\n",
    "    \"\"\"Applies dynamic quantization. Returns CPU model.\"\"\"\n",
    "    try:\n",
    "        quantized_model = copy.deepcopy(model).cpu()\n",
    "        quantized_model.eval()\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            quantized_model, {nn.GRU, nn.Linear}, dtype=torch.qint8)\n",
    "        return quantized_model\n",
    "    except Exception as e:\n",
    "         builtin_print(f\"Error during dynamic quantization: {e}\")\n",
    "         return copy.deepcopy(model).cpu()\n",
    "\n",
    "class WeatherGRU(nn.Module):\n",
    "    \"\"\"GRU model definition.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, output_dim, dropout):\n",
    "        super(WeatherGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers,\n",
    "                          batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "print(\"Utility functions and WeatherGRU class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be397e6b",
   "metadata": {},
   "source": [
    "# 4. Load Baseline Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the required artifacts: processed test data and the baseline model state.\n",
    "\n",
    "# Load processed test data tensors\n",
    "try:\n",
    "    processed_data = torch.load(Config.PROCESSED_DATA_PATH, map_location=torch.device('cpu')) # Load directly to CPU\n",
    "    X_test_tensor = processed_data['X_test']\n",
    "    y_test_tensor = processed_data['y_test']\n",
    "    builtin_print(f\"Processed test data loaded from {Config.PROCESSED_DATA_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    builtin_print(f\"Error: Processed data file not found at {Config.PROCESSED_DATA_PATH}. Please run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    builtin_print(f\"Error loading processed data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create Test DataLoader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Use a larger batch size for potentially faster evaluation if memory allows\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.EVAL_BATCH_SIZE * 2, shuffle=False)\n",
    "builtin_print(\"Test DataLoader created.\")\n",
    "\n",
    "# Initialize the GRU model structure\n",
    "baseline_model_structure = WeatherGRU(\n",
    "    input_dim=Config.INPUT_DIM,\n",
    "    hidden_dim=Config.HIDDEN_DIM,\n",
    "    n_layers=Config.N_LAYERS,\n",
    "    output_dim=Config.OUTPUT_DIM,\n",
    "    dropout=Config.DROPOUT\n",
    ")\n",
    "\n",
    "# Load the trained baseline model state dictionary onto the CPU\n",
    "try:\n",
    "    baseline_model_structure.load_state_dict(torch.load(Config.BASELINE_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "    baseline_model_structure.eval() # Set to evaluation mode\n",
    "    builtin_print(f\"Baseline model state loaded from {Config.BASELINE_MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    builtin_print(f\"Error: Baseline model file not found at {Config.BASELINE_MODEL_PATH}. Please run train_baseline.ipynb first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    builtin_print(f\"Error loading baseline model state: {e}\")\n",
    "    raise\n",
    "\n",
    "# Keep a clean copy of the loaded baseline model on the CPU\n",
    "baseline_model_cpu = baseline_model_structure.cpu()\n",
    "print(\"Baseline model ready on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b6f05",
   "metadata": {},
   "source": [
    "# 5. Determine Best Action from Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b70792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the best action details (pruning ratio, quantization) determined by the RL agent.\n",
    "# It prioritizes loading from the JSON file saved by train_rl_agent.ipynb.\n",
    "# If the JSON is unavailable, it attempts to load the saved PPO agent and predict the action.\n",
    "\n",
    "best_action_info = None\n",
    "\n",
    "# --- Attempt 1: Load from JSON file ---\n",
    "try:\n",
    "    with open(Config.BEST_ACTION_PATH, 'r') as f:\n",
    "        best_action_info = json.load(f)\n",
    "        # Basic validation of the loaded data\n",
    "        if 'pruning_amount' in best_action_info and 'quantized' in best_action_info:\n",
    "             builtin_print(f\"Best action info successfully loaded from {Config.BEST_ACTION_PATH}:\")\n",
    "             builtin_print(f\"  Source: {best_action_info.get('source', 'Unknown')}\")\n",
    "             builtin_print(f\"  Pruning: {best_action_info['pruning_amount']*100:.0f}%\")\n",
    "             builtin_print(f\"  Quantized: {best_action_info['quantized']}\")\n",
    "             if 'action_code' in best_action_info:\n",
    "                 builtin_print(f\"  Originating Action Code: {best_action_info['action_code']}\")\n",
    "        else:\n",
    "             builtin_print(f\"Warning: JSON file {Config.BEST_ACTION_PATH} is missing required keys ('pruning_amount', 'quantized'). Will attempt agent loading.\")\n",
    "             best_action_info = None # Invalidate if keys are missing\n",
    "except FileNotFoundError:\n",
    "    builtin_print(f\"Info: Best action JSON file not found at {Config.BEST_ACTION_PATH}. Proceeding to load agent for prediction.\")\n",
    "except json.JSONDecodeError:\n",
    "     builtin_print(f\"Error: Could not decode JSON from {Config.BEST_ACTION_PATH}. File might be corrupted. Proceeding to load agent.\")\n",
    "except Exception as e:\n",
    "    builtin_print(f\"Error loading best action JSON: {e}. Proceeding to load agent.\")\n",
    "\n",
    "# --- Attempt 2: Load Agent and Predict (Fallback) ---\n",
    "if best_action_info is None:\n",
    "    builtin_print(f\"\\n--- Attempting to load trained agent from {Config.AGENT_SAVE_PATH} to predict best action ---\")\n",
    "    try:\n",
    "        # Define necessary parameters for a minimal environment mock, needed for PPO.load()\n",
    "        dummy_pruning_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        dummy_action_space = spaces.Discrete(len(dummy_pruning_levels) * 2)\n",
    "        dummy_obs_space = spaces.Box(low=np.array([0.0, -1.0, 0.0, 0.0]), high=np.array([1.0, 1.0, 1.0, 1.0]), dtype=np.float32)\n",
    "\n",
    "        # Minimal mock environment class\n",
    "        class MockEnv(gym.Env):\n",
    "            def __init__(self):\n",
    "                super().__init__(); self.action_space = dummy_action_space; self.observation_space = dummy_obs_space\n",
    "            def step(self, action): pass\n",
    "            def reset(self, seed=None, options=None):\n",
    "                super().reset(seed=seed); return np.array([0.6, 0.0, 0.0, 0.0], dtype=np.float32), {} # Plausible initial state\n",
    "            def render(self): pass\n",
    "            def close(self): pass\n",
    "\n",
    "        mock_env_instance = MockEnv()\n",
    "\n",
    "        # Load the PPO agent onto the CPU\n",
    "        agent = PPO.load(Config.AGENT_SAVE_PATH, env=mock_env_instance, device='cpu')\n",
    "        builtin_print(\"Agent loaded successfully onto CPU.\")\n",
    "\n",
    "        # Get initial observation from the mock environment\n",
    "        obs, _ = mock_env_instance.reset()\n",
    "\n",
    "        # Predict the action deterministically using the loaded agent's policy\n",
    "        action_pred, _ = agent.predict(obs, deterministic=True)\n",
    "        action_code_pred = action_pred.item()\n",
    "        builtin_print(f\"Agent predicted best action code: {action_code_pred}\")\n",
    "\n",
    "        # Decode the predicted action code\n",
    "        n_pruning_levels = len(dummy_pruning_levels)\n",
    "        pruning_idx_pred = action_code_pred % n_pruning_levels\n",
    "        pruning_amount_pred = dummy_pruning_levels[pruning_idx_pred]\n",
    "        quantized_pred = action_code_pred >= n_pruning_levels\n",
    "\n",
    "        # Store the derived action information\n",
    "        best_action_info = {\n",
    "            'source': 'agent_prediction_fallback',\n",
    "            'action_code': action_code_pred,\n",
    "            'pruning_amount': pruning_amount_pred,\n",
    "            'quantized': quantized_pred\n",
    "        }\n",
    "        builtin_print(f\"  Decoded Action -> Pruning: {pruning_amount_pred*100:.0f}%, Quantized: {quantized_pred}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        builtin_print(f\"Error: Agent file not found at {Config.AGENT_SAVE_PATH}. Cannot evaluate agent-optimized solution.\")\n",
    "        best_action_info = None # Ensure it remains None if agent loading fails\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Error loading agent or predicting action: {e}. Cannot evaluate agent-optimized solution.\")\n",
    "        best_action_info = None\n",
    "\n",
    "# Final check\n",
    "if best_action_info is None:\n",
    "     builtin_print(\"\\nWarning: Could not determine the agent's best action from JSON or agent prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d15739",
   "metadata": {},
   "source": [
    "# 6. Run Full Benchmark Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function orchestrates the evaluation of four model versions:\n",
    "# 1. Baseline: The original trained GRU model.\n",
    "# 2. Manual Pruning (50%): Baseline model with 50% L1 pruning applied.\n",
    "# 3. Manual Quantization: Baseline model with dynamic quantization applied.\n",
    "# 4. Agent Optimized: Baseline model with the pruning/quantization strategy determined in the previous step.\n",
    "# It uses the `evaluate_model` function for consistent metric calculation on the CPU.\n",
    "\n",
    "def run_full_benchmark(baseline_model_cpu, test_loader, best_action_info):\n",
    "    \"\"\"Runs and aggregates benchmark results for different model versions.\"\"\"\n",
    "    results = {} # Dictionary to store metrics for each version\n",
    "    print(\"\\n--- Starting Full Benchmark Evaluation ---\")\n",
    "\n",
    "    # --- 1. Baseline Evaluation ---\n",
    "    print(\"\\n--- Evaluating: Baseline Model ---\")\n",
    "    try:\n",
    "        results['Baseline'] = evaluate_model(copy.deepcopy(baseline_model_cpu), test_loader, Config.CODECARBON_BATCHES)\n",
    "        print(\"  Baseline evaluation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating baseline: {e}\")\n",
    "        results['Baseline'] = {} # Store empty dict on error\n",
    "\n",
    "    # --- 2. Manual Pruning (50%) Evaluation ---\n",
    "    print(\"\\n--- Evaluating: Manual Pruning (50%) ---\")\n",
    "    try:\n",
    "        pruned_model_manual = apply_l1_pruning(baseline_model_cpu, 0.5)\n",
    "        results['Manual Pruning (50%)'] = evaluate_model(pruned_model_manual, test_loader, Config.CODECARBON_BATCHES)\n",
    "        print(\"  Manual pruning evaluation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating manual pruning: {e}\")\n",
    "        results['Manual Pruning (50%)'] = {}\n",
    "\n",
    "    # --- 3. Manual Quantization Evaluation ---\n",
    "    print(\"\\n--- Evaluating: Manual Quantization ---\")\n",
    "    try:\n",
    "        quantized_model_manual = apply_dynamic_quantization(baseline_model_cpu)\n",
    "        results['Manual Quantization'] = evaluate_model(quantized_model_manual, test_loader, Config.CODECARBON_BATCHES)\n",
    "        print(\"  Manual quantization evaluation complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating manual quantization: {e}\")\n",
    "        results['Manual Quantization'] = {}\n",
    "\n",
    "    # --- 4. Agent Optimized Evaluation ---\n",
    "    print(\"\\n--- Evaluating: Agent Optimized Model ---\")\n",
    "    if best_action_info and 'pruning_amount' in best_action_info and 'quantized' in best_action_info:\n",
    "        pruning_amount_agent = best_action_info['pruning_amount']\n",
    "        apply_quant_agent = best_action_info['quantized']\n",
    "        action_code_agent = best_action_info.get('action_code', 'N/A') # Get action code if available\n",
    "        print(f\"  Applying Agent's strategy (Action: {action_code_agent}): Pruning {pruning_amount_agent*100:.0f}%, Quantized: {apply_quant_agent}\")\n",
    "        try:\n",
    "            agent_optimized_model = copy.deepcopy(baseline_model_cpu) # Start fresh\n",
    "            if pruning_amount_agent > 0:\n",
    "                agent_optimized_model = apply_l1_pruning(agent_optimized_model, pruning_amount_agent)\n",
    "            if apply_quant_agent:\n",
    "                agent_optimized_model = apply_dynamic_quantization(agent_optimized_model)\n",
    "\n",
    "            # Evaluate the resulting agent-optimized model\n",
    "            results['Agent Optimized'] = evaluate_model(agent_optimized_model, test_loader, Config.CODECARBON_BATCHES)\n",
    "            print(\"  Agent optimized evaluation complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error evaluating agent optimized model: {e}\")\n",
    "            results['Agent Optimized'] = {}\n",
    "    else:\n",
    "        print(\"  Skipping agent evaluation: Best action information is not available or invalid.\")\n",
    "        results['Agent Optimized'] = {} # Ensure the key exists even if skipped\n",
    "\n",
    "    # --- Format Results into a DataFrame ---\n",
    "    df_results = pd.DataFrame(results).T.fillna(0) # Transpose and fill NaNs\n",
    "\n",
    "    # Calculate derived metrics (GFLOPs, MParams, mWh) safely\n",
    "    df_results['flops_g'] = (df_results['flops'] / 1e9).round(6) if 'flops' in df_results else 0.0\n",
    "    df_results['params_m'] = (df_results['params'] / 1e6).round(6) if 'params' in df_results else 0.0\n",
    "    df_results['energy_mwh'] = (df_results['energy_kwh'] * 1000).round(6) if 'energy_kwh' in df_results else 0.0\n",
    "\n",
    "    # Ensure all desired columns exist, adding missing ones with default 0.0\n",
    "    final_columns = ['accuracy', 'params_m', 'flops_g', 'energy_mwh', 'co2_eq_kg']\n",
    "    for col in final_columns:\n",
    "        if col not in df_results.columns:\n",
    "            df_results[col] = 0.0\n",
    "        # Ensure numeric type after potential fillna(0)\n",
    "        df_results[col] = pd.to_numeric(df_results[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Benchmark Evaluation Finished ---\")\n",
    "    # Return only the formatted columns of interest\n",
    "    return df_results[final_columns]\n",
    "\n",
    "# --- Execute the Benchmark ---\n",
    "benchmark_results_df = run_full_benchmark(baseline_model_cpu, test_loader, best_action_info)\n",
    "\n",
    "# --- Display the Results Table ---\n",
    "print(\"\\n\\n--- FINAL COMPREHENSIVE BENCHMARK RESULTS ---\")\n",
    "# Display using markdown for better rendering in notebooks\n",
    "print(benchmark_results_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622fdfc1",
   "metadata": {},
   "source": [
    "# 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates bar charts comparing accuracy, model size, computational cost, and energy consumption\n",
    "# across the different model versions. Also creates a scatter plot (Pareto front visualization)\n",
    "# showing the trade-off between accuracy and computational cost (FLOPs).\n",
    "\n",
    "print(\"\\n--- Generating Performance Comparison Visualizations ---\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\") # Use seaborn's whitegrid style\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 11)) # Create a 2x2 grid of plots\n",
    "fig.suptitle('Comprehensive Model Performance Comparison', fontsize=16, weight='bold')\n",
    "\n",
    "# Define plot details for each metric\n",
    "metrics_to_plot = [\n",
    "    {'metric': 'accuracy', 'ax': axes[0, 0], 'palette': 'Greens_r', 'title': 'Model Accuracy', 'ylabel': 'Accuracy Score'},\n",
    "    {'metric': 'params_m', 'ax': axes[0, 1], 'palette': 'Blues_r', 'title': 'Model Size (Millions of Parameters)', 'ylabel': 'Parameters (M)'},\n",
    "    {'metric': 'flops_g', 'ax': axes[1, 0], 'palette': 'Oranges_r', 'title': 'Computational Cost (GFLOPs)', 'ylabel': 'GFLOPs'},\n",
    "    {'metric': 'energy_mwh', 'ax': axes[1, 1], 'palette': 'Reds_r', 'title': 'Inference Energy Consumption (mWh)', 'ylabel': 'Energy (mWh)'}\n",
    "]\n",
    "\n",
    "# Create each bar plot\n",
    "for plot_info in metrics_to_plot:\n",
    "    metric = plot_info['metric']\n",
    "    ax = plot_info['ax']\n",
    "    if metric in benchmark_results_df.columns:\n",
    "        try:\n",
    "            # Generate bar plot using seaborn\n",
    "            sns.barplot(x=benchmark_results_df.index, y=metric, data=benchmark_results_df, ax=ax, palette=plot_info['palette'])\n",
    "            ax.set_title(plot_info['title'], fontsize=12, weight='bold')\n",
    "            ax.set_ylabel(plot_info['ylabel'])\n",
    "            ax.tick_params(axis='x', rotation=15, labelsize=10) # Rotate x-labels slightly\n",
    "            ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "            # Add value labels specifically for accuracy bars\n",
    "            if metric == 'accuracy':\n",
    "                 for index, value in enumerate(benchmark_results_df[metric]):\n",
    "                      ax.text(index, value + 0.005, f'{value:.4f}', ha='center', va='bottom', fontsize=9) # Position label slightly above bar\n",
    "\n",
    "            # Adjust y-axis limits for better visualization, handling zero values\n",
    "            min_val, max_val = benchmark_results_df[metric].min(), benchmark_results_df[metric].max()\n",
    "            if max_val > min_val: # If there's a range\n",
    "                 padding = (max_val - min_val) * 0.1 # Add 10% padding\n",
    "                 ax.set_ylim(bottom=max(0, min_val - padding), top=max_val + padding)\n",
    "            elif max_val > 0: # If all values are the same and positive\n",
    "                 ax.set_ylim(bottom=0, top=max_val * 1.2)\n",
    "            else: # If all values are zero or negative\n",
    "                 ax.set_ylim(bottom=min_val * 1.1 if min_val < 0 else -0.01, top=0.01)\n",
    "\n",
    "        except Exception as e:\n",
    "            builtin_print(f\"Warning: Could not plot {metric}: {e}\")\n",
    "            ax.set_title(f\"{plot_info['title']}\\\\n(Plotting Error)\", fontsize=12, weight='bold')\n",
    "    else:\n",
    "        ax.set_title(f\"{plot_info['title']}\\\\n(Data N/A)\", fontsize=12, weight='bold')\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust overall layout\n",
    "plt.show()\n",
    "\n",
    "# --- Generate Pareto Plot (Accuracy vs. FLOPs) ---\n",
    "print(\"\\n--- Generating Pareto Plot (Accuracy vs. Computational Cost) ---\")\n",
    "plt.figure(figsize=(10, 6)) # Adjust figure size\n",
    "\n",
    "if 'flops_g' in benchmark_results_df.columns and 'accuracy' in benchmark_results_df.columns:\n",
    "    try:\n",
    "        sns.scatterplot(\n",
    "            data=benchmark_results_df,\n",
    "            x='flops_g',\n",
    "            y='accuracy',\n",
    "            hue=benchmark_results_df.index,\n",
    "            s=120, # Marker size\n",
    "            style=benchmark_results_df.index,\n",
    "            palette='viridis'\n",
    "        )\n",
    "        plt.title('Accuracy vs. Computational Cost (GFLOPs) Trade-off', fontsize=14, weight='bold')\n",
    "        plt.xlabel('Computational Cost (GFLOPs)', fontsize=11)\n",
    "        plt.ylabel('Accuracy', fontsize=11)\n",
    "        plt.legend(title='Model Version', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        # Adjust plot limits for clarity, especially for near-zero FLOPs\n",
    "        max_flops = benchmark_results_df['flops_g'].max()\n",
    "        plt.xlim(left=-0.002, right=max_flops * 1.1 if max_flops > 0 else 0.002) # Start slightly left of 0\n",
    "\n",
    "        min_acc, max_acc = benchmark_results_df['accuracy'].min(), benchmark_results_df['accuracy'].max()\n",
    "        acc_padding = (max_acc - min_acc) * 0.1 if (max_acc - min_acc) > 0.01 else 0.01\n",
    "        plt.ylim(bottom=min_acc - acc_padding, top=max_acc + acc_padding)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        builtin_print(f\"Warning: Could not generate Pareto plot: {e}\")\n",
    "else:\n",
    "    builtin_print(\"Skipping Pareto plot: Required columns ('flops_g', 'accuracy') not found in results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
